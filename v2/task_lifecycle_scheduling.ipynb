{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16b752be-c1a2-428b-be5c-bcd3fbaeaf50",
   "metadata": {},
   "source": [
    "# Distributed Scheduling Deep-dive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9065e725-7bb2-4fe9-8a54-78dc1f52eee5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Ray Cluster overview\n",
    "\n",
    "A Ray cluster consits of:\n",
    "- One or more **woker nodes**, where each worker node consists of the following processes:\n",
    "    - **worker processes** responsible for task submission and execution. Each worker process stores:\n",
    "        - An **ownership table**: System metadata for the objects to which the worker has a reference, e.g., to store ref counts and object locations\n",
    "        - An **in-process store**: used to store small objects (<100KB)\n",
    "    - A **raylet**: Manages shared resources on each node. The raylet has two main components:\n",
    "        - A **scheduler**: Responsible for resource management, task placement, and fulfilling task arguments that are stored in the distributed object store. The individual schedulers in a cluster comprise **the Ray distributed scheduler**.\n",
    "        - A **shared-memory object store** (also known as the **Plasma Object Store**). Responsible for storing, transferring, and spilling **large objects**. The individual object stores in a cluster comprise the **Ray distributed object store**.\n",
    "- One of the worker nodes is designated as a **head node** which is a special node that also hosts\n",
    "  - A **global control service**: keeps track of the **cluster state** that is not supposed to change often\n",
    "  - **Cluster level services**: are services that are shared across the cluster suc as autoscaling, job submission, etc. \n",
    "  - A **driver**: a special worker process that executes the top-level application. It can submit tasks but does not execute them.\n",
    "\n",
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/ray_cluster.png\" width=\"800\">\n",
    "\n",
    "\n",
    "Note that \n",
    "- The **plasma object store** by default is in-memory and takes up **30% of the memory of the node**\n",
    "- If the **plasma object store** is full, objects are **spilled to disk**\n",
    "\n",
    "<!-- \n",
    "Reference: \n",
    "- See [V2 architecture document -> Architecture Overview -> Design -> Components](https://docs.google.com/document/d/1tBw9A4j62ruI5omIJbMxly-la5w4q_TjyJgJL_jN2fI/preview#heading=h.cclei73t0j5p)\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21758807-b7ab-4460-8464-758220161447",
   "metadata": {},
   "source": [
    "## Scheduling a Task\n",
    "\n",
    "Below is a high-level diagram showing the primary stages in scheduling a task leading to its execution. \n",
    "\n",
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/scheduling_high_level.svg\" width=\"1400px\">\n",
    "\n",
    "Note that the diagram presents a simplified ordering of the steps invovled. Also note that while it shows certain steps being launched from a worker or a raylet, those steps are run in separate processes as we will detail later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb2ee56-449e-40dd-8b10-fe99ff0ee1a5",
   "metadata": {},
   "source": [
    "### Stage 1: Serializing Function Code\n",
    "\n",
    "When a Ray task is first called, its definition is pickled and then stored in the GCS.\n",
    "\n",
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/scheduling_serialization.svg\" width=\"600px\">\n",
    "\n",
    "\n",
    "<!-- References:\n",
    "- See code:\n",
    "    1. [Python .remote calls ._remote](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/remote_function.py#L140)\n",
    "    2. [Python ._remote pickles the function](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/remote_function.py#L299)\n",
    "    3. [Python ._remote call exports the function via the function manager.export](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_private/function_manager.py#L273)\n",
    "    4. [Which calls the cython GcsClient.internal_kv_put](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L2579)\n",
    "    5. [Which calls the gcs_client.cc PythonGcsClient::InternalKVPut](https://github.com/ray-project/ray/blob/55ab6dfd6b415f8795dd1dfed7b3fde2558efc46/src/ray/gcs/gcs_client/gcs_client.cc#L312) that sets the key, value in the proper namespace -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392c3ad2-37ec-4578-b24f-8422dcc94dea",
   "metadata": {},
   "source": [
    "### Stage 2: Claiming ownership\n",
    "\n",
    "Ownership is claimed by following these steps:\n",
    "- The core worker process will request from its task manager to add the pending task\n",
    "- The task manager will create an object reference for the returned result\n",
    "- The core worker will request from its reference counter to update its ownership table with the created object reference\n",
    "\n",
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/scheduling_ownership_claim.svg\" width=\"600px\">\n",
    "\n",
    "<!-- References:\n",
    "- See code:\n",
    "    1. [Python .remote calls ._remote](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/remote_function.py#L140)\n",
    "    2. [python ._remote call calls submit_task](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/remote_function.py#L420)\n",
    "    3. [submit_task calls the cython submit_task function defined here](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L3574)\n",
    "    4. [cython submit_task Delegates to C++ CoreWorker::SubmitTask]((https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L3643)\n",
    "    5. [CoreWorker::SubmitTask in turn calls TaskManager::AddPendingTask](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/core_worker.cc#L1944)\n",
    "    6. [TaskManager::AddPendingTask assigns a return id for referencing the task result](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/task_manager.cc#L195]\n",
    "    7. [TaskManager::AddPendingTask calls ReferenceCounter::AddOwnedObject to claim ownership of task result](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/task_manager.cc#L207)\n",
    "  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c33592-b0c8-41da-98b1-10167654ca79",
   "metadata": {},
   "source": [
    "### Stage 3: Resolving Task Dependencies\n",
    "\n",
    "Given a particular task `Task` that depends on:\n",
    "- object `A` as input\n",
    "- object `B` as input\n",
    "\n",
    "The core worker's submitter process will perform these steps\n",
    "\n",
    "1. Find each dependency's owner:\n",
    "    - Inspect the object reference to object `A` to find its owner address (IP + port)\n",
    "    - Inspect the object reference to object `B` to find its owner address (IP + port)\n",
    "2. Fetch metadata from the owner's ownership table about each object\n",
    "    - Request metadata about object `A` (e.g. the size and location of the of the object)\n",
    "    - Request metadata about object `B` (e.g. the size and location of the of the object)\n",
    "3. Proceed with scheduling now that all dependencies are resolved\n",
    "\n",
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/scheduling_resolving_deps.svg\" width=\"600px\">\n",
    "\n",
    "<!-- References:\n",
    "- See code:\n",
    "    1. [Python .remote calls ._remote](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/remote_function.py#L140)\n",
    "    2. [python ._remote call calls submit_task](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/remote_function.py#L420)\n",
    "    3. [submit_task calls the cython submit_task function defined here](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L3574)\n",
    "    4. [cython submit_task Delegates to C++ CoreWorker::SubmitTask]((https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L3643)\n",
    "    5. [CoreWorker::SubmitTask calls direct_task_submitter.SubmitTask](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/core_worker.cc#L1949)\n",
    "    5. [direct_task_submitter.SubmitTask calls ResolveDependencies to resolve dependencies](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/transport/direct_task_transport.cc#L28)\n",
    "    6. [ResolveDependencies calls InlineDependencies](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/transport/dependency_resolver.cc#L117)\n",
    "    7. [InlineDependencies fetches task metadata like the size](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/transport/dependency_resolver.cc#L44C10-L44C10)\n",
    "  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fd54b7-7647-46bd-a8e4-cbbdc16b9f4d",
   "metadata": {},
   "source": [
    "### Stage 4: Enforcing data locality\n",
    "\n",
    "The core worker's submitter process will choose the raylet on the node that has the most number of object argument bytes already local to schedule the task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ed1749-21e2-47f0-bc71-cbdab818684c",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/scheduling_data_locality.svg\" width=\"800px\">\n",
    "\n",
    "Note: this stage is skipped in case the task's specified scheduling policy is stringent (e.g. a node-affinity policy)\n",
    "\n",
    "<!-- References:\n",
    "- See code:\n",
    "    1. [Python .remote calls ._remote](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/remote_function.py#L140)\n",
    "    2. [python ._remote call calls submit_task](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/remote_function.py#L420)\n",
    "    3. [submit_task calls the cython submit_task function](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L3643)\n",
    "    4. [cython submit_task Delegates to SubmitTask from c++ Core Worker with calls direct_task_submitter.SubmitTask](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/core_worker.cc#L1949)\n",
    "    5. [direct_task_submitter.SubmitTask calls ResolveDependencies to resolve dependencies](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/transport/direct_task_transport.cc#L28)\n",
    "    6. [direct_task_submitter.SubmitTask as a callback will now call RequestNewWorkerIfNeeded](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/transport/direct_task_transport.cc#L135)\n",
    "    7. [RequestNewWorkerIfNeeded will in turn call GetBestNodeForTask](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/transport/direct_task_transport.cc#L394)\n",
    "    8. [GetBestNodeForTask will pick a node for locality in case the scheduling strategy is not stringent (i.e. node affinity or spread)](https://github.com/ray-project/ray/blob/master/src/ray/core_worker/lease_policy.cc#L39)\n",
    "    9. [GetBestNodeIdForTask will find the node with the most object bytes](https://github.com/ray-project/ray/blob/master/src/ray/core_worker/lease_policy.cc#L47C1-L48C1) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7d6fc2-ee57-46b0-ba48-8dd1ee529c08",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Stage 5: Applying Scheduling Policy\n",
    "\n",
    "The selected \"Scheduler Raylet\" will undergo these steps to locate the best node to execute the task\n",
    "\n",
    "- It receives updates from the GCS every 100ms\n",
    "    - The updates contain metadata about the resources on each node\n",
    "- It determines which nodes/raylets are feasible for scheduling given the task resource requirements\n",
    "- It will then apply the task's scheduling policy\n",
    "- The policy will determine the node/raylet that will be executing the task (i.e. our \"executor raylet\")\n",
    "\n",
    "\n",
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/scheduling_applying_policy.svg\" width=\"600px\">\n",
    "\n",
    "<!-- References:\n",
    "- See code:\n",
    "    1. Add task to task queue:\n",
    "        1. [Python .remote calls ._remote](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/remote_function.py#L140)\n",
    "        2. [python ._remote call calls submit_task](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/remote_function.py#L420)\n",
    "        3. [submit_task calls the cython submit_task function](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L3643)\n",
    "        4. [cython submit_task Delegates to SubmitTask from c++ Core Worker with calls direct_task_submitter.SubmitTask](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/core_worker.cc#L1949)\n",
    "        5. [direct_task_submitter.SubmitTask calls ResolveDependencies to resolve dependencies](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/transport/direct_task_transport.cc#L28)\n",
    "        6. [direct_task_submitter.SubmitTask will now schedule the task onto the task queue](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/transport/direct_task_transport.cc#L113)\n",
    "    2. Raylet's scheduler picks up task from task queue and applies scheduling policy\n",
    "        3. [A raylet when instantiated is composed of a node manager](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/raylet.h#L96)\n",
    "        4. A node manager is composed of:\n",
    "            - A cluster task manager which is responsible for queuing, spilling back, and dispatching tasks.\n",
    "            - A cluster resource scheduler is responsible for maintaining a view of the cluster state w.r.t resource usage.\n",
    "            - [These two classes make up the distributed scheduler](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/node_manager.h#L819)\n",
    "        5. [direct_task_submitter.SubmitTask as a callback will now call RequestNewWorkerIfNeeded](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/transport/direct_task_transport.cc#L135)\n",
    "        6. [RequestNewWorkerIfNeeded will in turn call RequestWorkerLease](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/transport/direct_task_transport.cc#L405)\n",
    "        6. [When a worker lease request comes in a raylet's NodeManager::HandleRequestWorkerLease gets call](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/node_manager.cc#L1783)\n",
    "        7. [NodeManager::HandleRequestWorkerLease will delegate a call to ClusterTaskManager::QueueAndScheduleTask()](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/node_manager.cc#L1830)\n",
    "        8. [ClusterTaskManager::QueueAndScheduleTask it will delegate a call to ClusterTaskManager::ScheduleAndDispatchTasks()](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/scheduling/cluster_task_manager.cc#L64)\n",
    "        9. [ScheduleAndDispatchTasks will call ClusterResourceManager::GetBestSchedulableNode() taking into account the schedulding strategy set on the task specification](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/scheduling/cluster_task_manager.cc#L148C14-L155C1) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7ce746-8f30-479e-a7a3-764400be2866",
   "metadata": {},
   "source": [
    "### Stage 6: Leasing a worker\n",
    "\n",
    "The executor raylet will perform these steps for each task in its queue\n",
    "- It will try to ensure a fair distribution of tasks amongst different scheduling policies\n",
    "- It will wait for the tasks arguments to be available\n",
    "- It will check if the node can allocate resources for the task\n",
    "- It will now secure a worker and respond with the worker address to the owner process\n",
    "\n",
    "If it fails to any of the checks, it will try to:\n",
    "- wait and retry the steps\n",
    "- re-apply the scheduling policy to check if it can spill the task to another node that is now better suited\n",
    "\n",
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/scheduling_leasing_worker.svg\" width=\"1000px\">\n",
    "\n",
    "Notes: \n",
    "- After this stage, the owner process will now own the lease to the worker\n",
    "- The lease remains active as long as the owner process and leased worker are alive, and the raylet ensures that no other client may use the worker while the lease is active.\n",
    "- The owner may schedule any number of tasks onto the leased worker, as long as the tasks are compatible with the granted resource request. Hence, leases can be thought of as an optimization to avoid communication with the scheduler for similar scheduling requests.\n",
    "\n",
    "\n",
    "<!-- References:\n",
    "- See code:\n",
    "    1. [When a worker lease request comes in a raylet's NodeManager::HandleRequestWorkerLease gets call](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/node_manager.cc#L1783)\n",
    "    2. [NodeManager::HandleRequestWorkerLease will delegate a call to ClusterTaskManager::QueueAndScheduleTask()](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/node_manager.cc#L1830)\n",
    "    3. [ClusterTaskManager::QueueAndScheduleTask it will delegate a call to ClusterTaskManager::ScheduleAndDispatchTasks()](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/scheduling/cluster_task_manager.cc#L64)\n",
    "    4. [ClusterTaskManager.ScheduleAndDispatchTasks() will call LocalTaskManager::ScheduleAndDispatchTasks()](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/scheduling/cluster_task_manager.cc#L227)\n",
    "    5. [LocalTaskManager::ScheduleAndDispatchTasks() will call LocalTaskManager::DispatchScheduledTasksToWorkers()](https://github.com/ray-project/ray/blob/master/src/ray/raylet/local_task_manager.cc#L96)\n",
    "    6. [LocalTaskManager::DispatchScheduledTasksToWorkers() will secure a worker for a task in the call to WorkPool.PopWorker() only after securing that owner is active, arguments are available, resources are allocated)](https://github.com/ray-project/ray/blob/master/src/ray/raylet/local_task_manager.cc#L267)\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf77f07-e97c-4a49-853d-9b6aebb5a743",
   "metadata": {},
   "source": [
    "### Stage 7: Executing the task\n",
    "\n",
    "Once a worker receives a task it will perform these steps:\n",
    "\n",
    "- Consume the task from the queue\n",
    "- Prepare the function for execution\n",
    "- Submit the RayFunction to a task execution callback that will\n",
    "    - Deserialize the function arguments\n",
    "    - Construct and execute the python function within cython (dealing with async functions)\n",
    "    - Handle results properly:\n",
    "        - Perform error handling extracting error information if there is a failure\n",
    "        - Deal with generator results\n",
    "    - Prepare the return object\n",
    "        - Seal the return object\n",
    "        - Update the reference counts\n",
    "        - Run garbage collection:\n",
    "            - if \"borrowed\" dependency objects are no longer needed, they will be deleted from the object store\n",
    "        - If the return object(s) are small\n",
    "            - Return the values inline directly to the owner, which copies them to its in-process object store.\n",
    "        - If the return object(s) are large\n",
    "            - Store the objects in the node's shared memory store and reply to  owner indicating the objects are now in distributed memory.\n",
    "- Return the final status of the task execution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dadf2b-c39d-40c0-a649-3f12d2ac0879",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/scheduling_execution.svg\" width=\"1200px\">\n",
    "\n",
    "\n",
    "<!-- References: \n",
    "- See code:\n",
    "    1. [When instantiating a CoreWorker, we add task receivers which will callback CoreWorker::ExecuteTask](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/core_worker.cc#L147)\n",
    "    2. [CoreWorker::ExecuteTask() will prepare a RayFunction and submit it to its execution callback](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/core_worker.cc#L2721C21-L2721C44)\n",
    "    3. [The task execution callback in the case of python will execute the function from cython given the set task_execution_handler](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L3075C43-L3075C65)\n",
    "    4. [The task execution handler will execute the task with a cancellation handler](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L2064C17-L2064C55)\n",
    "    5. [The handler will call execute_task handling a KeyboardInterrupt error](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L1960C7-L1960C7)\n",
    "    6. [execute_task will invoke the function_executor](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L1675)\n",
    "    7. [execute_task will store the outputs in the object store](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L1810C12-L1810C12) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bebdb9-23bf-4f5c-b75c-7dddb5a139b1",
   "metadata": {},
   "source": [
    "### Stage 8: Fetching the task result\n",
    "\n",
    "Here are the steps in fetching a task's result, i.e. when ray.get gets called\n",
    "\n",
    "1. find the owner using the object reference\n",
    "2. the owner will look up its ownership table to find where in distributed memory the object is located\n",
    "3. the owner will fetch the object and return it\n",
    "4. the owner will update the reference count to the object\n",
    "5. the owner will trigger grabage collection if the reference count is now 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4b01e6-ab1f-45ae-ab14-50734fbccf22",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/scheduling_fetching.svg\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d18be57-14d8-4ac8-bc97-9dd2103f5b83",
   "metadata": {},
   "source": [
    "## Scheduling Policies Deep-dive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0946813d-9312-4e63-9872-7d9c9e14bc96",
   "metadata": {},
   "source": [
    "We will now attempt to understand the available scheduling policies in ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ee7a35-f5d1-428d-a3e1-a66c4407fb98",
   "metadata": {},
   "source": [
    "### Classifying nodes as feasible/infeasible and available/unavailable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3774958c-07fa-4e62-be46-54a18ddd0fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Task</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Node</th>\n",
       "      <th>Raylet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Resource Requirements</th>\n",
       "      <th>Resource Capacity</th>\n",
       "      <th>Status</th>\n",
       "      <th>Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>num_cpus=3</td>\n",
       "      <td>4 CPUs</td>\n",
       "      <td>Idle node</td>\n",
       "      <td>Feasible and Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>num_cpus=3</td>\n",
       "      <td>4 CPUs</td>\n",
       "      <td>2 CPUs tied up running another task</td>\n",
       "      <td>Feasible but Not Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>num_cpus=3</td>\n",
       "      <td>2CPUs</td>\n",
       "      <td>Idle node</td>\n",
       "      <td>Infeasible</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Task              Node  \\\n",
       "  Resource Requirements Resource Capacity   \n",
       "0            num_cpus=3            4 CPUs   \n",
       "1            num_cpus=3            4 CPUs   \n",
       "2            num_cpus=3             2CPUs   \n",
       "\n",
       "                                                            Raylet  \n",
       "                                Status              Classification  \n",
       "0                            Idle node      Feasible and Available  \n",
       "1  2 CPUs tied up running another task  Feasible but Not Available  \n",
       "2                            Idle node                  Infeasible  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read DataFrame from CSV\n",
    "pd.read_csv(\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/raylet_node_classification.csv\", header=[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9755b82f-b86d-4e47-ac4f-5c9238dca42c",
   "metadata": {},
   "source": [
    "#### Default Hybrid policy\n",
    "\n",
    "This is the default policy used by ray. It is a hybrid policy that combines the following two heuristics:\n",
    "- Bin packing heuristic\n",
    "- Load balancing heuristic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa50e575-7810-4762-92c5-ba307c64f1bb",
   "metadata": {},
   "source": [
    "The diagram below shows the policy in action in a bin-packing heuristic/mode\n",
    "\n",
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/scheduling_policy_hybrid_policy_binpacking.svg\" width=\"900px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637ed34d-1d44-4b58-bbd0-3641f7c365cd",
   "metadata": {},
   "source": [
    "The diagram below shows the policy in action in a load balancing heuristic\n",
    "\n",
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/scheduling_policy_hybrid_policy_balancing.svg\" width=\"900px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01228a05-2425-4e50-811c-3e3f06f22a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
