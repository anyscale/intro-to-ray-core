{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16b752be-c1a2-428b-be5c-bcd3fbaeaf50",
   "metadata": {},
   "source": [
    "# Deep-dive into the ray task's lifecycle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964a4ab2-0760-4a1b-83de-00c08a27433e",
   "metadata": {},
   "source": [
    "We start by visualizing a task's execution using the following diagram:\n",
    "\n",
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/task_execution_detail_0.svg\" width=\"800px\">\n",
    "\n",
    "In case you skipped it, this same diagram was presented in our high-level overview of ray tasks.\n",
    "\n",
    "We will proceed to add more color to this diagram providing useful details for each step of the process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9065e725-7bb2-4fe9-8a54-78dc1f52eee5",
   "metadata": {},
   "source": [
    "## Small diversion: what are the main components of a ray cluster ?\n",
    "\n",
    "A Ray cluster consists of:\n",
    "- One or more **worker nodes**, where each worker node consists of the following processes:\n",
    "    - **worker processes** responsible for task submission and execution.\n",
    "    - A **raylet** responsible for resource management and task placement.\n",
    "- One of the worker nodes is designated a **head node** and is responsible for running \n",
    "  - A **global control service** responsible for keeping track of the **cluster-level state** that is not supposed to change too frequently.\n",
    "\n",
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/ray_cluster_detail_0.svg\" width=\"800px\">\n",
    "\n",
    "<!-- \n",
    "Reference: \n",
    "- See [V2 architecture document -> Architecture Overview -> Design -> Components](https://docs.google.com/document/d/1tBw9A4j62ruI5omIJbMxly-la5w4q_TjyJgJL_jN2fI/preview#heading=h.cclei73t0j5p)\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1837554-89d5-46bb-9343-96d2bc125518",
   "metadata": {},
   "source": [
    "## Task Execution: Component attribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2bdf5e-5d3b-4dca-a922-51c90fe84e24",
   "metadata": {},
   "source": [
    "Now that we are familiar with the different components on a ray cluster, here is our same tax execution diagram revisited with colors indicating which component is responsible for each step.\n",
    "\n",
    "- One **worker process** submits the task\n",
    "- The cluster **autoscaler** will hanlde upscaling nodes to meet new resource requirements\n",
    "- **Raylet(s)** will handle task scheduling/placement on a worker\n",
    "- **One worker process** executes the task\n",
    "- The result information is sent back to the **submitter worker** once complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313528c7-5685-4828-a5da-f45efcdbb186",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/task_execution_detail_1.svg\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e771bd14-45ab-4a01-ac9d-a85f246acb12",
   "metadata": {},
   "source": [
    "## Task Execution: Exporting and Loading Function Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9e8d8f-54c3-49d8-84f7-b073e06dba5c",
   "metadata": {},
   "source": [
    "Remember a task wraps around a given function - in python a task decorates a python function.\n",
    "\n",
    "- The submitter worker will serialize the function definition\n",
    "    - In the case of python, ray makes use of a variant of pickle (cloudpickle) to serialize the function\n",
    "- The submitter worker will then export the function definition to the GCS Store\n",
    "- The executor worker will then load and cache the function definition from the GCS Store\n",
    "- The executor worker will then deserialize the code and execute the function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff498cea-99fc-4619-b295-c5e1ee0fc6e8",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/task_execution_detail_export_load.svg\" width=\"900px\">\n",
    "\n",
    "<!-- References:\n",
    "- See code:\n",
    "    - Exporting Function to GCS Store\n",
    "        1. [Python .remote calls ._remote](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/remote_function.py#L140)\n",
    "        2. [Python ._remote pickles the function](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/remote_function.py#L299)\n",
    "        3. [Python ._remote call exports the function via the function manager.export](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_private/function_manager.py#L273)\n",
    "        4. [Which calls the cython GcsClient.internal_kv_put](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L2579)\n",
    "        5. [Which calls the gcs_client.cc PythonGcsClient::InternalKVPut](https://github.com/ray-project/ray/blob/55ab6dfd6b415f8795dd1dfed7b3fde2558efc46/src/ray/gcs/gcs_client/gcs_client.cc#L312) that sets the key, value in the proper namespace \n",
    "    - Importing Function from GCS Store\n",
    "        1. [When instantiating a CoreWorker, we add task receivers which will callback CoreWorker::ExecuteTask](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/core_worker.cc#L147)\n",
    "        2. [CoreWorker::ExecuteTask() will prepare a RayFunction and submit it to its execution callback](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/core_worker.cc#L2721C21-L2721C44)\n",
    "        3. [The task execution callback in the case of python will execute the function from cython given the set task_execution_handler](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L3075C43-L3075C65)\n",
    "        4. [The task execution handler will execute the task with a cancellation handler](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L2064C17-L2064C55)\n",
    "        5. [The handler will call the function_manager.get_execution_info](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L1944)\n",
    "        6. [function_manager.get_execution_info will in turn call function_manager._wait_for_function](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_private/function_manager.py#L393)\n",
    "        7. [function_manager._wait_for_function will in turn call function_manager.fetch_and_register_remote_function](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_private/function_manager.py#L455C33-L455C67)\n",
    "        8. [function_manager.fetch_and_register_remote_function will in turn call function_manager.fetch_registered_method](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_private/function_manager.py#L299C37-L299C60)\n",
    "        9. [function_manager.fetch_registered_method will in turn call gcs_client.internal_kv_get to read the function defintion from the GCS KV store](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_private/function_manager.py#L281)\n",
    "    - Caching the function definition:\n",
    "        10. [As a continuation the execution_infos in-memory dictionary mapping is updated to store the function definition] (https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L1946)\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b76c87-2f76-4b49-94ef-b41b45c608e7",
   "metadata": {},
   "source": [
    "## Task Execution: Submission - Resolving Dependencies and Data Locality "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310873ef-d5df-4c8e-95f5-0997bd698384",
   "metadata": {},
   "source": [
    "Here are some key steps in task submission:\n",
    "\n",
    "- A submitter worker won't request a task to be executed prior to resolving its dependencies.\n",
    "- A submitter worker will chose the worker node that has most of the dependency data local to it.\n",
    "- A submitter worker will request what ray calls a \"Worker Lease\" from the raylet on the data-locality-optimal node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552d341d-8471-48af-b1b8-a9c68a04d37d",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/task_execution_detail_resolving_deps_data_locality.svg\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063e5323-3bd8-4cf9-aa9e-5b2db5520623",
   "metadata": {},
   "source": [
    "let's unpack the above steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0de1b1-0887-4d50-b2eb-f07b834eeda7",
   "metadata": {},
   "source": [
    "### Resolving Task Dependencies\n",
    "\n",
    "Given a particular task `task1` that depends on, objects `A` and `B` as inputs\n",
    "\n",
    "The submitter worker process will perform these two main steps\n",
    "\n",
    "1. Wait for each object to be available via async callbacks\n",
    "    - remember `A` and `B` could very well be the outputs of a different task, hence why we need to wait \n",
    "2. Proceed with scheduling now that all dependencies are resolved\n",
    "\n",
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/resolving_deps.svg\" width=\"600px\">\n",
    "\n",
    "<!-- References:\n",
    "- See code:\n",
    "    1. [Python .remote calls ._remote](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/remote_function.py#L140)\n",
    "    2. [python ._remote call calls submit_task](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/remote_function.py#L420)\n",
    "    3. [submit_task calls the cython submit_task function defined here](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L3574)\n",
    "    4. [cython submit_task Delegates to C++ CoreWorker::SubmitTask]((https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L3643)\n",
    "    5. [CoreWorker::SubmitTask calls direct_task_submitter.SubmitTask](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/core_worker.cc#L1949)\n",
    "    5. [direct_task_submitter.SubmitTask calls ResolveDependencies to resolve dependencies](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/transport/direct_task_transport.cc#L28)\n",
    "    6. [ResolveDependencies calls InlineDependencies](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/transport/dependency_resolver.cc#L117)\n",
    "    7. [InlineDependencies fetches task metadata like the size](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/transport/dependency_resolver.cc#L44C10-L44C10)\n",
    "  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59848ed9-8bd9-410e-93ed-262838613072",
   "metadata": {},
   "source": [
    "The submitter process will choose the node that has the **most number of object argument bytes** already local.\n",
    "\n",
    "The diagram shows the same particular task `task1` we saw before. \n",
    "\n",
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/data_locality.svg\" width=\"600px\">\n",
    "\n",
    "Note: \"enforcing data locality\" stage is skipped in case the task's specified scheduling policy is stringent (e.g. a node-affinity policy) - scheduling policies will be discussed in more detail later.\n",
    "\n",
    "<!-- References:\n",
    "- See code:\n",
    "    1. [Python .remote calls ._remote](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/remote_function.py#L140)\n",
    "    2. [python ._remote call calls submit_task](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/remote_function.py#L420)\n",
    "    3. [submit_task calls the cython submit_task function](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L3643)\n",
    "    4. [cython submit_task Delegates to SubmitTask from c++ Core Worker with calls direct_task_submitter.SubmitTask](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/core_worker.cc#L1949)\n",
    "    5. [direct_task_submitter.SubmitTask calls ResolveDependencies to resolve dependencies](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/transport/direct_task_transport.cc#L28)\n",
    "    6. [direct_task_submitter.SubmitTask as a callback will now call RequestNewWorkerIfNeeded](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/transport/direct_task_transport.cc#L135)\n",
    "    7. [RequestNewWorkerIfNeeded will in turn call GetBestNodeForTask](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/transport/direct_task_transport.cc#L394)\n",
    "    8. [GetBestNodeForTask will pick a node for locality in case the scheduling strategy is not stringent (i.e. node affinity or spread)](https://github.com/ray-project/ray/blob/master/src/ray/core_worker/lease_policy.cc#L39)\n",
    "    9. [GetBestNodeIdForTask will find the node with the most object bytes](https://github.com/ray-project/ray/blob/master/src/ray/core_worker/lease_policy.cc#L47C1-L48C1) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e540aed-7af5-4274-9a4a-c1193c308ca9",
   "metadata": {},
   "source": [
    "## Task Execution: Autoscaling nodes given resource requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889ac2c6-5d9c-43ff-b156-0ce783861550",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/task_execution_autoscaling.svg\" width=\"900px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ab9247-07f5-4f45-a317-fd226e729a38",
   "metadata": {},
   "source": [
    "## Task Execution: Scheduling - Finding The Best Node and Allocating Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce112e4-a12a-4725-930b-0ab9608922c7",
   "metadata": {},
   "source": [
    "Now that a worker lease request is sent, here are the steps that follow to schedule a task\n",
    "\n",
    "- The raylet on the data-locality-optimal node receives the worker lease request\n",
    "    - It receives a view of the entire cluster state from the GCS via a periodic broadcast\n",
    "    - It makes a decision on which node is the best based on its view of the cluster state\n",
    "- The Raylet on the best node now has to allocate the resources to lease a worker\n",
    "    - It will attempt to reserve the resources on the node\n",
    "    - It will then update the GCS periodically with any updates about the resource state of the node\n",
    " \n",
    "This is shown in the below diagram, the potential autoscaling step prior to finding a best node is left out to simplify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334bdd22-9963-4369-a7a6-0212baca2810",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/task_execution_detail_find_best_node_allocate_resources.svg\" width=\"900px\">\n",
    "\n",
    "<!-- References:\n",
    "- See code:\n",
    "    1. [When a worker lease request comes in a raylet's NodeManager::HandleRequestWorkerLease gets called](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/node_manager.cc#L1783)\n",
    "    2. [NodeManager::HandleRequestWorkerLease will delegate a call to ClusterTaskManager::QueueAndScheduleTask()](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/node_manager.cc#L1830)\n",
    "    3. [ClusterTaskManager::QueueAndScheduleTask it will delegate a call to ClusterTaskManager::ScheduleAndDispatchTasks()](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/scheduling/cluster_task_manager.cc#L64)\n",
    "    4. [ClusterTaskManager.ScheduleAndDispatchTasks() will call LocalTaskManager::ScheduleAndDispatchTasks()](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/scheduling/cluster_task_manager.cc#L227)\n",
    "    5. [LocalTaskManager::ScheduleAndDispatchTasks() will call LocalTaskManager::DispatchScheduledTasksToWorkers()](https://github.com/ray-project/ray/blob/master/src/ray/raylet/local_task_manager.cc#L96)\n",
    "    6. [LocalTaskManager::DispatchScheduledTasksToWorkers() will secure a worker for a task in the call to WorkPool.PopWorker() only after securing that owner is active, arguments are available, resources are allocated)](https://github.com/ray-project/ray/blob/master/src/ray/raylet/local_task_manager.cc#L267)\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5863d0d3-a7be-4d2d-bc44-5fdec1615639",
   "metadata": {},
   "source": [
    "### Leases as an optimization to avoid communication with the scheduler for similar scheduling requests\n",
    "\n",
    "- A scheduling request at task submission can reuse a leased worker if it has the same:\n",
    "    - Resource requirements as these must be acquired from the node during task execution.\n",
    "    - Shared-memory task arguments, as these must be made local on the node before task execution.\n",
    "- This \"hot path\" most commonly occurs for **subsequent task executions**. We visualize it in the diagram below. Note how we skip:\n",
    "    - sending a request to a raylet altogether\n",
    "    - storing and fetching the function code in GCS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1e8c3e-4950-4afb-884c-c36e8657c800",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/task_execution_scheduling_hot_path.svg\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab2dccb-3420-482f-8d7a-53dbbe10c074",
   "metadata": {},
   "source": [
    "## Task Execution: Object Handling (Storage and Distributed Ownership)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b73d55-40c2-4af8-b04b-d62ea8012224",
   "metadata": {},
   "source": [
    "Let's revist our mental model for the ray cluster and add some more detail to which components control and manage objects in ray.\n",
    "\n",
    "- Each worker process stores:\n",
    "    - An ownership table. System metadata for the objects to which the worker has a reference, e.g., to store ref counts and object locations.\n",
    "    - An in-process store, used to store small objects.\n",
    "- Each raylet runs:\n",
    "    - A shared-memory object store (also known as the Plasma Object Store). Responsible for storing, transferring, and spilling large objects. The individual object stores in a cluster comprise the Ray distributed object store.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e73764b-947b-4a7e-a0a4-00850d3a189d",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/ray_cluster_distributed_ownership.svg\" width=\"800px\">\n",
    "\n",
    "<!-- \n",
    "Reference: \n",
    "- See [V2 architecture document -> Architecture Overview -> Design -> Components](https://docs.google.com/document/d/1tBw9A4j62ruI5omIJbMxly-la5w4q_TjyJgJL_jN2fI/preview#heading=h.cclei73t0j5p)\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1da0a9-572b-460c-9ea6-a1ddda00bcca",
   "metadata": {},
   "source": [
    "Let's take a look at the steps involved in object handling:\n",
    "\n",
    "- The submitter worker creates an object reference for the future output value of the task in its ownership table\n",
    "- The submitter worker then submits the task for scheduling\n",
    "- The executor worker will execute the task function\n",
    "- The executor worker will then prepare the return object\n",
    "    - If the return object is small <100KB\n",
    "        - Return the values inline directly to the submitter's in-process object store.\n",
    "    - If the return object is large\n",
    "        - Store the objects in the raylet object store\n",
    "- Executor updates the submitter's ownership table with reference to new object address"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33201b6-6850-47ff-97da-48dbb9c82313",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/task_execution_detail_distributed_object_store_ownership.svg\" width=\"800px\">\n",
    "\n",
    "<!-- References: \n",
    "- See code:\n",
    "    1. [When instantiating a CoreWorker, we add task receivers which will callback CoreWorker::ExecuteTask](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/core_worker.cc#L147)\n",
    "    2. [CoreWorker::ExecuteTask() will prepare a RayFunction and submit it to its execution callback](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/core_worker.cc#L2721C21-L2721C44)\n",
    "    3. [The task execution callback in the case of python will execute the function from cython given the set task_execution_handler](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L3075C43-L3075C65)\n",
    "    4. [The task execution handler will execute the task with a cancellation handler](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L2064C17-L2064C55)\n",
    "    5. [The handler will call execute_task handling a KeyboardInterrupt error](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L1960C7-L1960C7)\n",
    "    6. [execute_task will invoke the function_executor](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L1675)\n",
    "    7. [execute_task will store the outputs in the object store](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L1810C12-L1810C12) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a2ca85-2a78-467c-aa56-f80cf23c40a6",
   "metadata": {},
   "source": [
    "## Distributed ownership work in ray\n",
    "\n",
    "### How does it work ?\n",
    "The process that submits a task is considered to be the owner of the result of the task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65c4990-a466-4044-8f81-edc649a8d3e5",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/distributed_ownership_overview.svg\" width=\"600px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89923e26-9910-43a6-a991-757f07917d0d",
   "metadata": {},
   "source": [
    "## Upsides to distributed ownership:\n",
    "\n",
    "- Latency: Faster than communicating all ownership information back to a head node.\n",
    "- Scalability: There is no central bottleneck when attempting to scale the cluster given every worker maintains its own ownership information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48a0ba6-de22-4168-971a-ea0b8f465026",
   "metadata": {},
   "source": [
    "## Downsides to distributed ownership:\n",
    "\n",
    "- objects fate-share with their owner. Even though the object is available on a node, if the owner fails, the object is no longer reachable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f19405b-3a65-4b01-9670-60dbaf33b3c3",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/distributed_ownership_fate_share_with_owner.svg\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a04790-a38f-4e56-aa8a-cdcbbc795dcf",
   "metadata": {},
   "source": [
    "## Distributed Object Store\n",
    "\n",
    "The raylet's object store can be thought of as shared memory across all workers on a node.\n",
    "\n",
    "For values that can be zero-copy deserialized, passing the ObjectRef to `ray.get` or as a task argument will return a direct pointer to the shared memory buffer to the worker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8fc3c1-f86e-4157-86d6-73a93e41acfe",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/distributed_ownership_data_sharing.svg\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc916b30-6ab0-47e0-b587-b6db4b43e4dd",
   "metadata": {},
   "source": [
    "### Downside to a shared object-store\n",
    "\n",
    "This also means that worker processes fate-share with their local raylet process.\n",
    "\n",
    "A simple mental model to have is `raylet = node` if a raylet fails, all workloads on node will fail "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a11ba7-2e48-49f0-9f8c-f17dbc4d3e79",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/distributed_ownership_fate_share_with_raylet.svg\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d18be57-14d8-4ac8-bc97-9dd2103f5b83",
   "metadata": {},
   "source": [
    "# Overview of Scheduling Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0946813d-9312-4e63-9872-7d9c9e14bc96",
   "metadata": {},
   "source": [
    "Ray provides different scheduling strategies that you can set on your task.\n",
    "\n",
    "We will go over:\n",
    "- How a raylet assess feasibility and availability of nodes\n",
    "- How every scheduling strategy/policy works and when you should use it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ee7a35-f5d1-428d-a3e1-a66c4407fb98",
   "metadata": {},
   "source": [
    "## How does a raylet classify nodes as feasible/infeasible and available/unavailable?\n",
    "\n",
    "Given a resource requirement, a raylet classifies a node as one of the following:\n",
    "- feasible\n",
    "    - available\n",
    "    - not available\n",
    "- infeasible node \n",
    "\n",
    "Let's understand this by looking at an example task `my_task` that has a resource requirement of 3 CPUs:\n",
    "\n",
    "- all nodes with >= 3 CPUs are classified as **feasible**\n",
    "    - all **feasible nodes** that have >= 3 CPUs **idle** are classified as **available**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007b8d8d-cb3e-49c9-baf9-55aae5a1b8bd",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/raylet_node_classification.svg\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9755b82f-b86d-4e47-ac4f-5c9238dca42c",
   "metadata": {},
   "source": [
    "## Default Scheduling Strategy\n",
    "\n",
    "This is the default scheduling policy used by ray\n",
    "\n",
    "### Motivation\n",
    "\n",
    "Ray attempts to strike a balance between favoring nodes that already cater for data locality and favoring those that have low resource utilization.\n",
    "\n",
    "### How does it work?\n",
    "It is a hybrid policy that combines the following two heuristics:\n",
    "- Bin packing heuristic\n",
    "- Load balancing heuristic\n",
    "\n",
    "<!-- ### References:\n",
    "- See code here:\n",
    "    - [Default Hybrid Scheduling Policy is defined here](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/scheduling/policy/hybrid_scheduling_policy.cc) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa50e575-7810-4762-92c5-ba307c64f1bb",
   "metadata": {},
   "source": [
    "The diagram below shows the policy in action in a bin-packing heuristic/mode\n",
    "\n",
    "Note the **Local Node** shown in the diagram is the node that is local to the raylet that received the worker lease request - which in almost all cases is the raylet that satisfies data locality requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc93fb1-0036-4328-902a-fccb85a4afa8",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/scheduling_policy_hybrid_policy_binpacking.svg\" width=\"900px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637ed34d-1d44-4b58-bbd0-3641f7c365cd",
   "metadata": {},
   "source": [
    "The diagram below shows the policy in action in a load balancing heuristic. \n",
    "\n",
    "This occurs when our preferred local node is heavily being utilized. The strategy will now spread new tasks amongst other feasible and available nodes.\n",
    "\n",
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/scheduling_policy_hybrid_policy_balancing.svg\" width=\"900px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1358afc-669c-4ea7-a873-e5747fbec9a2",
   "metadata": {},
   "source": [
    "## Node Affinity Strategy\n",
    "\n",
    "### How does it work?\n",
    "It assigns a task to a given node in either a strict or soft manner.\n",
    "\n",
    "### Use-cases\n",
    "- When you want to ensure that your task runs on a specific node: e.g. you want to make sure a given accelerator is used for a compute-intensive task.\n",
    "\n",
    "\n",
    "<!-- ### References:\n",
    "- See code here\n",
    "    - [Node Affinity Policy is defined here](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/scheduling/policy/node_affinity_scheduling_policy.cc)\n",
    "  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e958ad-4d1f-4513-be26-524b9d3e7958",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/scheduling_policy_node_affinity.svg\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66136a5-980f-455c-b97a-28e29597a49a",
   "metadata": {},
   "source": [
    "### Sample code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61371ae5-ffa8-48a0-b4e0-be5378c7874a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 15:03:45,916\tINFO worker.py:1633 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "from ray.util.scheduling_strategies import NodeAffinitySchedulingStrategy\n",
    "\n",
    "# pin this task to only run on the current node id\n",
    "run_on_same_node = NodeAffinitySchedulingStrategy(\n",
    "    node_id=ray.get_runtime_context().get_node_id(), \n",
    "    soft=False,\n",
    ")\n",
    "\n",
    "@ray.remote(\n",
    "    scheduling_strategy=run_on_same_nodeÏ\n",
    ")\n",
    "def node_affinity_schedule():\n",
    "    return 2\n",
    "\n",
    "\n",
    "ray.get(node_affinity_schedule.remote())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2d5f03-2742-45e0-9939-e4300c98e748",
   "metadata": {},
   "source": [
    "## SPREAD Scheduling Strategy\n",
    "\n",
    "### How does it work?\n",
    "It behaves like a best-effort round-robin. It spreads across all the available nodes first and then the feasible nodes.\n",
    "\n",
    "### Use-cases\n",
    "- When you want to load-balance your tasks across nodes. e.g. you are building a web service and want to avoid overloading certain nodes.\n",
    "\n",
    "\n",
    "<!-- ### References:\n",
    "- See code here\n",
    "    - [Spread Scheduling Policy is defined here](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/scheduling/policy/spread_scheduling_policy.cc)\n",
    "  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c1a5e4-2aae-462d-b4c3-f71280568ec5",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/scheduling_policy_spread.svg\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfbf117-865f-4b6f-88f6-6c9671ffa172",
   "metadata": {},
   "source": [
    "### Sample Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1610bdc8-ab5e-4de4-9a03-12f6609a0687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "\n",
    "@ray.remote(scheduling_strategy=\"SPREAD\")\n",
    "def spread_default_func():\n",
    "    return 2\n",
    "\n",
    "\n",
    "ray.get(spread_default_func.remote())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbf3515-9c48-42a3-a61f-83d722dc7aee",
   "metadata": {},
   "source": [
    "## Placement Group Scheduling Strategy\n",
    "\n",
    "In cases when we want to treat a set of resources as a single unit, we can use placement groups.\n",
    "\n",
    "### How does it work?\n",
    "\n",
    "- A **placement group** is formed from a set of **resource bundles**\n",
    "  - A **resource bundle** is a list of resource requirements that fit in a single node\n",
    "- A **placement group** can specify a **placement strategy** that determines how the **resource bundles** are placed\n",
    "  - The **placement strategy** can be one of the following:\n",
    "    - **PACK**: pack the **resource bundles** into as few nodes as possible\n",
    "    - **SPREAD**: spread the **resource bundles** across as many nodes as possible\n",
    "    - **STRICT_PACK**: pack the **resource bundles** into as few nodes as possible and fail if not possible\n",
    "    - **STRICT_SPREAD**: spread the **resource bundles** across as many nodes as possible and fail if not possible\n",
    "- **Placement Groups** are **atomic** \n",
    "  -  i.e. either all the **resource bundles** are placed or none are placed\n",
    "  -  GCS uses a two-phase commit protocol to ensure atomicity\n",
    "\n",
    "### Use-cases\n",
    "\n",
    "Placement groups are used for **atomic gang scheduling**. Imagine the use case of a distributed training that requires 4 GPU nodes total. Other distributed schedulers might first reserve 3 GPUs and hang waiting for the fourth hogging resources in the meantime. Ray, instead, will either reserve all 4 GPUs or it will fail scheduling.\n",
    "\n",
    "- Use SPREAD when you want to load-balance your tasks across nodes. e.g. you are building a web service and want to avoid overloading certain nodes.\n",
    "- Use PACK when you want to maximize resource utilization. e.g. you are running training and want to cut costs by packing all your resource bundles on a small subset of nodes.\n",
    "\n",
    "<!-- ### References\n",
    "- [See code here for Bundle Scheduling Policy](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/scheduling/policy/bundle_scheduling_policy.cc) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd86af99-9c06-4a9c-b73b-0c9f1d1cbe94",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/scheduling_policy_placement_group.svg\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9553a1dc-a980-42f6-a542-48974140578a",
   "metadata": {},
   "source": [
    "### Example Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fbf1929-0bcc-4abf-a372-e09eea922907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'placement_group_id': '1f68b1aaa61cf3a4130f9bd5ec2c01000000', 'name': 'my_pg', 'bundles': {0: {'CPU': 0.1}}, 'bundles_to_node_id': {0: 'ed50ecd2dcb4443e107b5b0b9da78065e86e8428924fd1ae82c0a780'}, 'strategy': 'PACK', 'state': 'CREATED', 'stats': {'end_to_end_creation_latency_ms': 2.57, 'scheduling_latency_ms': 2.349, 'scheduling_attempt': 1, 'highest_retry_delay_ms': 0.0, 'scheduling_state': 'FINISHED'}}\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.util.scheduling_strategies import PlacementGroupSchedulingStrategy\n",
    "# Import placement group related functions\n",
    "from ray.util.placement_group import (\n",
    "    placement_group,\n",
    "    placement_group_table,\n",
    "    remove_placement_group,\n",
    ")\n",
    "\n",
    "# Reserve a placement group of 1 bundle that reserves 0.1 CPU\n",
    "pg = placement_group([{\"CPU\": 0.1}], strategy=\"PACK\", name=\"my_pg\")\n",
    "\n",
    "# Wait until placement group is created.\n",
    "ray.get(pg.ready(), timeout=10)\n",
    "\n",
    "# look at placement group states using the table\n",
    "print(placement_group_table(pg))\n",
    "\n",
    "\n",
    "@ray.remote(\n",
    "    scheduling_strategy=PlacementGroupSchedulingStrategy(\n",
    "        placement_group=pg,\n",
    "    ),\n",
    "    # task requirement needs to be less than placement group capacity\n",
    "    num_cpus=0.1,\n",
    ")\n",
    "def placement_group_schedule():\n",
    "    return 2\n",
    "\n",
    "\n",
    "out = ray.get(placement_group_schedule.remote())\n",
    "print(out)\n",
    "\n",
    "# Remove placement group.\n",
    "remove_placement_group(pg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619893f8-b458-4e8a-a2de-d438120150db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
