{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16b752be-c1a2-428b-be5c-bcd3fbaeaf50",
   "metadata": {},
   "source": [
    "# Distributed Scheduling Deep-dive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9065e725-7bb2-4fe9-8a54-78dc1f52eee5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Ray Cluster overview\n",
    "\n",
    "A Ray cluster consists of:\n",
    "- One or more **woker nodes**, where each worker node consists of the following processes:\n",
    "    - **worker processes** responsible for task submission and execution. Each worker process stores:\n",
    "        - An **ownership table**: System metadata for the objects to which the worker has a reference, e.g., to store ref counts and object locations\n",
    "        - An **in-process store**: used to store small objects (<100KB)\n",
    "    - A **raylet**: Manages shared resources on each node. The raylet has two main components:\n",
    "        - A **scheduler**: Responsible for resource management, task placement, and fulfilling task arguments that are stored in the distributed object store. The individual schedulers in a cluster comprise **the Ray distributed scheduler**.\n",
    "        - A **shared-memory object store** (also known as the **Plasma Object Store**). Responsible for storing, transferring, and spilling **large objects**. The individual object stores in a cluster comprise the **Ray distributed object store**.\n",
    "- One of the worker nodes is designated as a **head node** which is a special node that also hosts\n",
    "  - A **global control service**: keeps track of the **cluster state** that is not supposed to change often\n",
    "  - **Cluster level services**: are services that are shared across the cluster suc as autoscaling, job submission, etc. \n",
    "  - A **driver**: a special worker process that executes the top-level application. It can submit tasks but does not execute them.\n",
    "\n",
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/ray_cluster.png\" width=\"800\">\n",
    "\n",
    "\n",
    "Note that \n",
    "- The **plasma object store** by default is in-memory and takes up **30% of the memory of the node**\n",
    "- If the **plasma object store** is full, objects are **spilled to disk**\n",
    "\n",
    "<!-- \n",
    "Reference: \n",
    "- See [V2 architecture document -> Architecture Overview -> Design -> Components](https://docs.google.com/document/d/1tBw9A4j62ruI5omIJbMxly-la5w4q_TjyJgJL_jN2fI/preview#heading=h.cclei73t0j5p)\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21758807-b7ab-4460-8464-758220161447",
   "metadata": {},
   "source": [
    "## Scheduling a Task\n",
    "\n",
    "Below is a high-level diagram of a task's lifecycle showing the primary stages in scheduling a task leading to its execution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24e3b3b-22ab-441b-b42b-9ac97ef94ed1",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/task_lifecycle_simplified.svg\" width=\"800px\">\n",
    "\n",
    "It primarily covers these three phases \n",
    "\n",
    "- Task Creation and Submission (Phase 1):\n",
    "    - Initialize: Instantiate a Core Worker (submitter) Process.\n",
    "    - Prepare: Serialize and store function code, and resolve dependencies\n",
    "    - Schedule: Determine the node that satisfies data locality and request a worker lease from its raylet.\n",
    "- Task Scheduling (Phase 2):\n",
    "    - Handle the Worker Lease Request: Apply the scheduling policy to find the optimal node.\n",
    "    - Allocate Resources: Raylet on optimal node will attempt to secure a lease by reserving necessary resources.\n",
    "- Task Execution and Result Handling (Phase 3):\n",
    "    - Execute: Run the task's code by the leased worker/executor process.\n",
    "    - Store Results:\n",
    "        - Send back small results (<100KB) to in-process memory of submitter.\n",
    "        - Store large results in local node's raylet object store and send back reference to submitter.\n",
    "     \n",
    "Note that the diagram presents a simplified ordering of the steps invovled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb2ee56-449e-40dd-8b10-fe99ff0ee1a5",
   "metadata": {},
   "source": [
    "### Serializing Function Code\n",
    "\n",
    "When a Ray task is first called, its definition is pickled and then stored in the GCS.\n",
    "\n",
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/scheduling_serialization_simplified.svg\" width=\"600px\">\n",
    "\n",
    "\n",
    "<!-- References:\n",
    "- See code:\n",
    "    1. [Python .remote calls ._remote](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/remote_function.py#L140)\n",
    "    2. [Python ._remote pickles the function](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/remote_function.py#L299)\n",
    "    3. [Python ._remote call exports the function via the function manager.export](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_private/function_manager.py#L273)\n",
    "    4. [Which calls the cython GcsClient.internal_kv_put](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L2579)\n",
    "    5. [Which calls the gcs_client.cc PythonGcsClient::InternalKVPut](https://github.com/ray-project/ray/blob/55ab6dfd6b415f8795dd1dfed7b3fde2558efc46/src/ray/gcs/gcs_client/gcs_client.cc#L312) that sets the key, value in the proper namespace -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c33592-b0c8-41da-98b1-10167654ca79",
   "metadata": {},
   "source": [
    "### Resolving Task Dependencies\n",
    "\n",
    "Given a particular task `Task` that depends on:\n",
    "- object `A` as input\n",
    "- object `B` as input\n",
    "\n",
    "The core worker submitter process will perform these two main steps\n",
    "\n",
    "1. Wait for each depenedncy to be available\n",
    "2. Proceed with scheduling now that all dependencies are resolved\n",
    "\n",
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/scheduling_resolving_deps_simplified.svg\" width=\"600px\">\n",
    "\n",
    "<!-- References:\n",
    "- See code:\n",
    "    1. [Python .remote calls ._remote](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/remote_function.py#L140)\n",
    "    2. [python ._remote call calls submit_task](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/remote_function.py#L420)\n",
    "    3. [submit_task calls the cython submit_task function defined here](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L3574)\n",
    "    4. [cython submit_task Delegates to C++ CoreWorker::SubmitTask]((https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L3643)\n",
    "    5. [CoreWorker::SubmitTask calls direct_task_submitter.SubmitTask](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/core_worker.cc#L1949)\n",
    "    5. [direct_task_submitter.SubmitTask calls ResolveDependencies to resolve dependencies](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/transport/direct_task_transport.cc#L28)\n",
    "    6. [ResolveDependencies calls InlineDependencies](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/transport/dependency_resolver.cc#L117)\n",
    "    7. [InlineDependencies fetches task metadata like the size](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/transport/dependency_resolver.cc#L44C10-L44C10)\n",
    "  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fd54b7-7647-46bd-a8e4-cbbdc16b9f4d",
   "metadata": {},
   "source": [
    "### Determine Data Locality\n",
    "\n",
    "The submitter process will choose the node that has the most number of object argument bytes already local.\n",
    "\n",
    "The diagram shows the same particular task `Task` we saw before. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ed1749-21e2-47f0-bc71-cbdab818684c",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/scheduling_data_locality_simplified.svg\" width=\"600px\">\n",
    "\n",
    "Note: this stage is skipped in case the task's specified scheduling policy is stringent (e.g. a node-affinity policy)\n",
    "\n",
    "<!-- References:\n",
    "- See code:\n",
    "    1. [Python .remote calls ._remote](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/remote_function.py#L140)\n",
    "    2. [python ._remote call calls submit_task](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/remote_function.py#L420)\n",
    "    3. [submit_task calls the cython submit_task function](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L3643)\n",
    "    4. [cython submit_task Delegates to SubmitTask from c++ Core Worker with calls direct_task_submitter.SubmitTask](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/core_worker.cc#L1949)\n",
    "    5. [direct_task_submitter.SubmitTask calls ResolveDependencies to resolve dependencies](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/transport/direct_task_transport.cc#L28)\n",
    "    6. [direct_task_submitter.SubmitTask as a callback will now call RequestNewWorkerIfNeeded](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/transport/direct_task_transport.cc#L135)\n",
    "    7. [RequestNewWorkerIfNeeded will in turn call GetBestNodeForTask](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/transport/direct_task_transport.cc#L394)\n",
    "    8. [GetBestNodeForTask will pick a node for locality in case the scheduling strategy is not stringent (i.e. node affinity or spread)](https://github.com/ray-project/ray/blob/master/src/ray/core_worker/lease_policy.cc#L39)\n",
    "    9. [GetBestNodeIdForTask will find the node with the most object bytes](https://github.com/ray-project/ray/blob/master/src/ray/core_worker/lease_policy.cc#L47C1-L48C1) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7d6fc2-ee57-46b0-ba48-8dd1ee529c08",
   "metadata": {},
   "source": [
    "### Handle Worker Lease Request\n",
    "\n",
    "We note the following steps to handle a worker lease request:\n",
    "\n",
    "- Every raylet receives updates from the GCS on a 100ms interval\n",
    "    - The updates contain metadata about the resources on each node\n",
    "- The Raylet on our data local node will now receive a worker lease request.\n",
    "- The Raylet finds the best node using the scheduling policy\n",
    "- The Raylet on the best node now has to allocate the resources to lease a worker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2328af36-890b-4edd-bd0e-2a6380eb280c",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/scheduling_applying_policy_simplified.svg\" width=\"600px\">\n",
    "\n",
    "<!-- References:\n",
    "- See code:\n",
    "    1. Add task to task queue:\n",
    "        1. [Python .remote calls ._remote](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/remote_function.py#L140)\n",
    "        2. [python ._remote call calls submit_task](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/remote_function.py#L420)\n",
    "        3. [submit_task calls the cython submit_task function](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L3643)\n",
    "        4. [cython submit_task Delegates to SubmitTask from c++ Core Worker with calls direct_task_submitter.SubmitTask](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/core_worker.cc#L1949)\n",
    "        5. [direct_task_submitter.SubmitTask calls ResolveDependencies to resolve dependencies](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/transport/direct_task_transport.cc#L28)\n",
    "        6. [direct_task_submitter.SubmitTask will now schedule the task onto the task queue](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/transport/direct_task_transport.cc#L113)\n",
    "    2. Raylet's scheduler picks up task from task queue and applies scheduling policy\n",
    "        3. [A raylet when instantiated is composed of a node manager](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/raylet.h#L96)\n",
    "        4. A node manager is composed of:\n",
    "            - A cluster task manager which is responsible for queuing, spilling back, and dispatching tasks.\n",
    "            - A cluster resource scheduler is responsible for maintaining a view of the cluster state w.r.t resource usage.\n",
    "            - [These two classes make up the distributed scheduler](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/node_manager.h#L819)\n",
    "        5. [direct_task_submitter.SubmitTask as a callback will now call RequestNewWorkerIfNeeded](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/transport/direct_task_transport.cc#L135)\n",
    "        6. [RequestNewWorkerIfNeeded will in turn call RequestWorkerLease](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/transport/direct_task_transport.cc#L405)\n",
    "        6. [When a worker lease request comes in a raylet's NodeManager::HandleRequestWorkerLease gets call](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/node_manager.cc#L1783)\n",
    "        7. [NodeManager::HandleRequestWorkerLease will delegate a call to ClusterTaskManager::QueueAndScheduleTask()](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/node_manager.cc#L1830)\n",
    "        8. [ClusterTaskManager::QueueAndScheduleTask it will delegate a call to ClusterTaskManager::ScheduleAndDispatchTasks()](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/scheduling/cluster_task_manager.cc#L64)\n",
    "        9. [ScheduleAndDispatchTasks will call ClusterResourceManager::GetBestSchedulableNode() taking into account the schedulding strategy set on the task specification](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/scheduling/cluster_task_manager.cc#L148C14-L155C1) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7ce746-8f30-479e-a7a3-764400be2866",
   "metadata": {},
   "source": [
    "### Leasing a worker\n",
    "\n",
    "The raylet on our best scheduleable node will now perform these steps to lease a worker:\n",
    "\n",
    "- It will wait for the resources to be available\n",
    "- If it passes the resource checks:\n",
    "    - It will now secure a worker and respond with the worker address to the owner process\n",
    "- If it fails to any of the checks, it will:\n",
    "    - wait and retry the scheduling\n",
    "\n",
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/scheduling_allocating_resources_simplified_.svg\" width=\"800px\">\n",
    "\n",
    "<!-- References:\n",
    "- See code:\n",
    "    1. [When a worker lease request comes in a raylet's NodeManager::HandleRequestWorkerLease gets call](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/node_manager.cc#L1783)\n",
    "    2. [NodeManager::HandleRequestWorkerLease will delegate a call to ClusterTaskManager::QueueAndScheduleTask()](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/node_manager.cc#L1830)\n",
    "    3. [ClusterTaskManager::QueueAndScheduleTask it will delegate a call to ClusterTaskManager::ScheduleAndDispatchTasks()](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/scheduling/cluster_task_manager.cc#L64)\n",
    "    4. [ClusterTaskManager.ScheduleAndDispatchTasks() will call LocalTaskManager::ScheduleAndDispatchTasks()](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/scheduling/cluster_task_manager.cc#L227)\n",
    "    5. [LocalTaskManager::ScheduleAndDispatchTasks() will call LocalTaskManager::DispatchScheduledTasksToWorkers()](https://github.com/ray-project/ray/blob/master/src/ray/raylet/local_task_manager.cc#L96)\n",
    "    6. [LocalTaskManager::DispatchScheduledTasksToWorkers() will secure a worker for a task in the call to WorkPool.PopWorker() only after securing that owner is active, arguments are available, resources are allocated)](https://github.com/ray-project/ray/blob/master/src/ray/raylet/local_task_manager.cc#L267)\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf77f07-e97c-4a49-853d-9b6aebb5a743",
   "metadata": {},
   "source": [
    "### Executing the task\n",
    "\n",
    "Once a worker receives a task it will perform these steps:\n",
    "\n",
    "- Prepare the function for execution: Fetch the serialized function code\n",
    "- Execute the function code\n",
    "- Prepare the return object\n",
    "    - If the return object(s) are small\n",
    "        - Return the values inline directly to the owner, which copies them to its in-process object store.\n",
    "    - If the return object(s) are large\n",
    "        - Store the objects in the node's shared memory store and reply to  owner indicating the objects are now in distributed memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dadf2b-c39d-40c0-a649-3f12d2ac0879",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/scheduling_execution_simplified.svg\" width=\"900px\">\n",
    "\n",
    "\n",
    "<!-- References: \n",
    "- See code:\n",
    "    1. [When instantiating a CoreWorker, we add task receivers which will callback CoreWorker::ExecuteTask](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/core_worker.cc#L147)\n",
    "    2. [CoreWorker::ExecuteTask() will prepare a RayFunction and submit it to its execution callback](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/core_worker.cc#L2721C21-L2721C44)\n",
    "    3. [The task execution callback in the case of python will execute the function from cython given the set task_execution_handler](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L3075C43-L3075C65)\n",
    "    4. [The task execution handler will execute the task with a cancellation handler](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L2064C17-L2064C55)\n",
    "    5. [The handler will call execute_task handling a KeyboardInterrupt error](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L1960C7-L1960C7)\n",
    "    6. [execute_task will invoke the function_executor](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L1675)\n",
    "    7. [execute_task will store the outputs in the object store](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L1810C12-L1810C12) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a2ca85-2a78-467c-aa56-f80cf23c40a6",
   "metadata": {},
   "source": [
    "## Distributed ownership work in ray\n",
    "\n",
    "### How does it work ?\n",
    "The process that submits a task is considered to be the owner of the result and is responsible for acquiring resources from a raylet to execute the task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65c4990-a466-4044-8f81-edc649a8d3e5",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/distributed_ownership_overview.svg\" width=\"600px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89923e26-9910-43a6-a991-757f07917d0d",
   "metadata": {},
   "source": [
    "## Upsides to distributed ownership:\n",
    "\n",
    "- Fault-tolerance: If a node where an object is stored fails, Ray will then attempt to recover the object through object reconstruction: Ray recovers the lost object through re-execution of the task that created the object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24a93de-2523-4654-a97e-d805533dfdc6",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/distributed_ownership_fault_tolerance.svg\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48a0ba6-de22-4168-971a-ea0b8f465026",
   "metadata": {},
   "source": [
    "## Downsides to distributed ownership:\n",
    "\n",
    "- objects fate-share with their owner. If the owner fails, the object is no longer reachable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f19405b-3a65-4b01-9670-60dbaf33b3c3",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/distributed_ownership_fate_share_with_owner.svg\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6848d24-70dd-4735-bd64-535971fa326f",
   "metadata": {},
   "source": [
    "### How does ray keep track of objects ?\n",
    "\n",
    "Ray makes use of ownership tables to record object locations. \n",
    "\n",
    "More specifically, here are is what happens:\n",
    "- At Task Submission:\n",
    "    - The submitter will create an object reference to the future resulting object.\n",
    "    - The object reference is then stored in the ownership table\n",
    "- Result Fetching\n",
    "    - The object reference is used to find the owner process\n",
    "    - The owner process then uses the ownership table to find the resulting object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eb7098-23fe-467d-a6a1-2fd987e278ce",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/distributed_ownership_ownership_table.svg\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a04790-a38f-4e56-aa8a-cdcbbc795dcf",
   "metadata": {},
   "source": [
    "## Distributed Object Store\n",
    "\n",
    "The raylet's object store can be thought of as shared memory across all workers on a node.\n",
    "\n",
    "For values that can be zero-copy deserialized, passing the ObjectRef to `ray.get` or as a task argument will return a direct pointer to the shared memory buffer to the worker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8fc3c1-f86e-4157-86d6-73a93e41acfe",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/distributed_ownership_data_sharing.svg\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc916b30-6ab0-47e0-b587-b6db4b43e4dd",
   "metadata": {},
   "source": [
    "### Downside to a shared object-store\n",
    "\n",
    "This also means that worker processes fate-share with their local raylet process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a11ba7-2e48-49f0-9f8c-f17dbc4d3e79",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/distributed_ownership_fate_share_with_raylet.svg\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d18be57-14d8-4ac8-bc97-9dd2103f5b83",
   "metadata": {},
   "source": [
    "# Overview of Scheduling Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0946813d-9312-4e63-9872-7d9c9e14bc96",
   "metadata": {},
   "source": [
    "Ray provides different scheduling strategies that you can set on your task.\n",
    "\n",
    "We will go over:\n",
    "- How a raylet assess feasibility and availability of nodes\n",
    "- How every scheduling strategy works and when you should use it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ee7a35-f5d1-428d-a3e1-a66c4407fb98",
   "metadata": {},
   "source": [
    "## How does a raylet classify nodes as feasible/infeasible and available/unavailable?\n",
    "\n",
    "Given a resource requirement, a raylet will determine if a node is\n",
    "- feasible vs infeasible node \n",
    "- for feasible nodes, the raylet determine if the node is:\n",
    "    - available or not available\n",
    "\n",
    "The raylet will make use of the resource updates that it receives by broadcast from GCS every 100ms to make these decisions.\n",
    "\n",
    "Let's understand this by looking at this example task `Task`, that has a resource requirement of 3 CPUs:\n",
    "- all nodes with >= 3 CPUs are classified as **feasible**, otherwise infeasible\n",
    "    - for those nodes that have >= 3 CPUs **idle**, they are classified as **available**, otherwise not available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3774958c-07fa-4e62-be46-54a18ddd0fc1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Task</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Node</th>\n",
       "      <th>Raylet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Resource Requirements</th>\n",
       "      <th>Resource Capacity</th>\n",
       "      <th>Status</th>\n",
       "      <th>Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>num_cpus=3</td>\n",
       "      <td>4 CPUs</td>\n",
       "      <td>Idle node</td>\n",
       "      <td>Feasible and Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>num_cpus=3</td>\n",
       "      <td>4 CPUs</td>\n",
       "      <td>2 CPUs tied up running another task</td>\n",
       "      <td>Feasible but Not Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>num_cpus=3</td>\n",
       "      <td>2CPUs</td>\n",
       "      <td>Idle node</td>\n",
       "      <td>Infeasible</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Task              Node  \\\n",
       "  Resource Requirements Resource Capacity   \n",
       "0            num_cpus=3            4 CPUs   \n",
       "1            num_cpus=3            4 CPUs   \n",
       "2            num_cpus=3             2CPUs   \n",
       "\n",
       "                                                            Raylet  \n",
       "                                Status              Classification  \n",
       "0                            Idle node      Feasible and Available  \n",
       "1  2 CPUs tied up running another task  Feasible but Not Available  \n",
       "2                            Idle node                  Infeasible  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read DataFrame from CSV\n",
    "pd.read_csv(\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/raylet_node_classification.csv\", header=[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9755b82f-b86d-4e47-ac4f-5c9238dca42c",
   "metadata": {},
   "source": [
    "## Default Scheduling Strategy\n",
    "\n",
    "### How does it work?\n",
    "This is the default policy used by ray. It is a hybrid policy that combines the following two heuristics:\n",
    "- Bin packing heuristic\n",
    "- Load balancing heuristic\n",
    "\n",
    "### Use-cases\n",
    "- Works well as a default: it usually results in data locality being honored.\n",
    "\n",
    "<!-- ### References:\n",
    "- See code here:\n",
    "    - [Default Hybrid Scheduling Policy is defined here](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/scheduling/policy/hybrid_scheduling_policy.cc) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa50e575-7810-4762-92c5-ba307c64f1bb",
   "metadata": {},
   "source": [
    "The diagram below shows the policy in action in a bin-packing heuristic/mode\n",
    "\n",
    "Note the **Local Node** shown in the diagram is the node that is local to the raylet that received the worker lease request - which in almost all cases is the raylet that satisfies data locality requirements.\n",
    "\n",
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/scheduling_policy_hybrid_policy_binpacking.svg\" width=\"900px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637ed34d-1d44-4b58-bbd0-3641f7c365cd",
   "metadata": {},
   "source": [
    "The diagram below shows the policy in action in a load balancing heuristic. \n",
    "\n",
    "This occurs when our preferred local node is heavily being utilized. The strategy will now spread new tasks amongst other feasible and available nodes.\n",
    "\n",
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/scheduling_policy_hybrid_policy_balancing.svg\" width=\"900px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1358afc-669c-4ea7-a873-e5747fbec9a2",
   "metadata": {},
   "source": [
    "## Node Affinity Strategy\n",
    "\n",
    "### How does it work?\n",
    "It assigns a task to a given node in either a strict or soft manner.\n",
    "\n",
    "### Use-cases\n",
    "- When you want to ensure that your task runs on a specific node: e.g. you want to make sure a given accelerator is used for a compute-intensive task.\n",
    "\n",
    "\n",
    "<!-- ### References:\n",
    "- See code here\n",
    "    - [Node Affinity Policy is defined here](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/scheduling/policy/node_affinity_scheduling_policy.cc)\n",
    "  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e958ad-4d1f-4513-be26-524b9d3e7958",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/scheduling_policy_node_affinity.svg\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66136a5-980f-455c-b97a-28e29597a49a",
   "metadata": {},
   "source": [
    "### Sample code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61371ae5-ffa8-48a0-b4e0-be5378c7874a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 15:03:45,916\tINFO worker.py:1633 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "from ray.util.scheduling_strategies import NodeAffinitySchedulingStrategy\n",
    "\n",
    "\n",
    "@ray.remote(\n",
    "    scheduling_strategy=NodeAffinitySchedulingStrategy(\n",
    "        node_id=ray.get_runtime_context().get_node_id(),\n",
    "        soft=False,\n",
    "    )\n",
    ")\n",
    "def node_affinity_schedule():\n",
    "    return 2\n",
    "\n",
    "\n",
    "ray.get(node_affinity_schedule.remote())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2d5f03-2742-45e0-9939-e4300c98e748",
   "metadata": {},
   "source": [
    "## SPREAD Scheduling Strategy\n",
    "\n",
    "### How does it work?\n",
    "It behaves like a best-effort round-robin. It spreads across all the available nodes first and then the feasible nodes.\n",
    "\n",
    "### Use-cases\n",
    "- When you want to load-balance your tasks across nodes. e.g. you are building a web service and want to avoid overloading certain nodes.\n",
    "\n",
    "\n",
    "<!-- ### References:\n",
    "- See code here\n",
    "    - [Spread Scheduling Policy is defined here](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/scheduling/policy/spread_scheduling_policy.cc)\n",
    "  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c1a5e4-2aae-462d-b4c3-f71280568ec5",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/scheduling_policy_spread.svg\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfbf117-865f-4b6f-88f6-6c9671ffa172",
   "metadata": {},
   "source": [
    "### Sample Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1610bdc8-ab5e-4de4-9a03-12f6609a0687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "\n",
    "@ray.remote(scheduling_strategy=\"SPREAD\")\n",
    "def spread_default_func():\n",
    "    return 2\n",
    "\n",
    "\n",
    "ray.get(spread_default_func.remote())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbf3515-9c48-42a3-a61f-83d722dc7aee",
   "metadata": {},
   "source": [
    "## Placement Group Scheduling Strategy\n",
    "\n",
    "In cases when we want to treat a set of resources as a single unit, we can use placement groups.\n",
    "\n",
    "### How does it work?\n",
    "\n",
    "- A **placement group** is formed from a set of **resource bundles**\n",
    "  - A **resource bundle** is a list of resource requirements that fit in a single node\n",
    "- A **placement group** can specify a **placement strategy** that determines how the **resource bundles** are placed\n",
    "  - The **placement strategy** can be one of the following:\n",
    "    - **PACK**: pack the **resource bundles** into as few nodes as possible\n",
    "    - **SPREAD**: spread the **resource bundles** across as many nodes as possible\n",
    "    - **STRICT_PACK**: pack the **resource bundles** into as few nodes as possible and fail if not possible\n",
    "    - **STRICT_SPREAD**: spread the **resource bundles** across as many nodes as possible and fail if not possible\n",
    "- **Placement Groups** are **atomic** \n",
    "  -  i.e. either all the **resource bundles** are placed or none are placed\n",
    "  -  GCS uses a two-phase commit protocol to ensure atomicity\n",
    "\n",
    "### Use-cases\n",
    "- Use SPREAD when you want to load-balance your tasks across nodes. e.g. you are building a web service and want to avoid overloading certain nodes.\n",
    "- Use PACK when you want to maximize resource utilization. e.g. you are running training and want to cut costs by packing all your resource bundles on a small subset of nodes.\n",
    "\n",
    "<!-- ### References\n",
    "- [See code here for Bundle Scheduling Policy](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/scheduling/policy/bundle_scheduling_policy.cc) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd86af99-9c06-4a9c-b73b-0c9f1d1cbe94",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/scheduling_policy_placement_group.svg\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9553a1dc-a980-42f6-a542-48974140578a",
   "metadata": {},
   "source": [
    "### Example Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fbf1929-0bcc-4abf-a372-e09eea922907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'placement_group_id': '1f68b1aaa61cf3a4130f9bd5ec2c01000000', 'name': 'my_pg', 'bundles': {0: {'CPU': 0.1}}, 'bundles_to_node_id': {0: 'ed50ecd2dcb4443e107b5b0b9da78065e86e8428924fd1ae82c0a780'}, 'strategy': 'PACK', 'state': 'CREATED', 'stats': {'end_to_end_creation_latency_ms': 2.57, 'scheduling_latency_ms': 2.349, 'scheduling_attempt': 1, 'highest_retry_delay_ms': 0.0, 'scheduling_state': 'FINISHED'}}\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.util.scheduling_strategies import PlacementGroupSchedulingStrategy\n",
    "# Import placement group related functions\n",
    "from ray.util.placement_group import (\n",
    "    placement_group,\n",
    "    placement_group_table,\n",
    "    remove_placement_group,\n",
    ")\n",
    "\n",
    "# Reserve a placement group of 1 bundle that reserves 0.1 CPU\n",
    "pg = placement_group([{\"CPU\": 0.1}], strategy=\"PACK\", name=\"my_pg\")\n",
    "\n",
    "# Wait until placement group is created.\n",
    "ray.get(pg.ready(), timeout=10)\n",
    "\n",
    "# look at placement group states using the table\n",
    "print(placement_group_table(pg))\n",
    "\n",
    "\n",
    "@ray.remote(\n",
    "    scheduling_strategy=PlacementGroupSchedulingStrategy(\n",
    "        placement_group=pg,\n",
    "    ),\n",
    "    # task requirement needs to be less than placement group capacity\n",
    "    num_cpus=0.1,\n",
    ")\n",
    "def placement_group_schedule():\n",
    "    return 2\n",
    "\n",
    "\n",
    "out = ray.get(placement_group_schedule.remote())\n",
    "print(out)\n",
    "\n",
    "# Remove placement group.\n",
    "remove_placement_group(pg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619893f8-b458-4e8a-a2de-d438120150db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
