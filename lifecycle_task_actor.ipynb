{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lifecycle of a task\n",
    "\n",
    "We start out detailing the full lifecylce of a task, from when it is created and submitted till when it is completed and the results are returned to the user. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10,000 feet view\n",
    "\n",
    "We have a python function convenitenly named `expensive_computation` which executes an expensive computation. To keep it simple all it does is perform a naive matrix multiplication and returns the number of elements in the resulting matrix. \n",
    "\n",
    "\n",
    "It gets called in sequence a number of times (`n_runs`) to be specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 10\n",
    "n = 300\n",
    "\n",
    "\n",
    "def perform_naive_matrix_multiplication(n):\n",
    "    # Create two n x n matrices\n",
    "    matrix1 = [[1 for _ in range(n)] for _ in range(n)]\n",
    "    matrix2 = [[1 for _ in range(n)] for _ in range(n)]\n",
    "\n",
    "    # Perform matrix multiplication\n",
    "    result = [[0 for _ in range(n)] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            for k in range(n):\n",
    "                result[i][j] += matrix1[i][k] * matrix2[k][j]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def expensive_computation(n):\n",
    "    result = perform_naive_matrix_multiplication(n)\n",
    "\n",
    "    # Return the number of elements in the result matrix\n",
    "    return len(result) * len(result[0])\n",
    "\n",
    "\n",
    "results = [expensive_computation(n) for _ in range(n_runs)]\n",
    "assert sum(results) == n_runs * n * n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the execution visualized\n",
    "\n",
    "<img src=\"sequential_simple_.jpeg\" height=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to:\n",
    "- Run the same function in a distributed fashion - i.e. in parallel on a cluster of machines\n",
    "- Get the results of the function as they become available\n",
    "\n",
    "We do this by following these steps:\n",
    "- Convert the `expensive_computation` function to a ray task decoration by decorating it with `ray.remote`\n",
    "- Submit a task for execution by calling `future = expensive_computation.remote()`\n",
    "- Use the returned `future` object reference to fetch the result of the function by calling `ray.get(future)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-13 20:37:05,883\tINFO worker.py:1633 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "\n",
    "@ray.remote  # decorator to convert python function to ray task\n",
    "def expensive_computation(n):\n",
    "    result = perform_naive_matrix_multiplication(n)\n",
    "    return len(result) * len(result[0])\n",
    "\n",
    "\n",
    "# submit n_run ray tasks to a ray cluster\n",
    "# and keep a reference to the task futures\n",
    "futures = [expensive_computation.remote(n) for _ in range(n_runs)]\n",
    "\n",
    "# wait for all tasks to complete and get the resulting objects\n",
    "results = ray.get(futures)\n",
    "\n",
    "# confirm that we got the right result\n",
    "assert sum(results) == n_runs * n * n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what is happening under the hood:\n",
    "\n",
    "<img src=\"parallel_simple.jpeg\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1000 feet view\n",
    "\n",
    "Let's detail the parallel execution of the function a bit more.\n",
    "\n",
    "More specifically:\n",
    "- ray tasks are executed on a ray cluster as part of a ray job\n",
    "- ray workers are the processes that execute the tasks\n",
    "- futures in ray are called `ObjectRef`s short for object references\n",
    "- results are stored as objects in the ray object store\n",
    "- ray.get() is used to fetch an object given its object reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "n = 300\n",
    "\n",
    "@ray.remote\n",
    "def expensive_computation(n):\n",
    "    result = perform_naive_matrix_multiplication(n)\n",
    "    return len(result) * len(result[0])\n",
    "\n",
    "# this returns an object reference to the result\n",
    "object_ref_future = expensive_computation.remote(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we inspect the object reference and see that it is a ray.ObjectRef with a given id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectRef(359ec6ce30d3ca2dffffffffffffffffffffffff0100000001000000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_ref_future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then call `ray.get` which waits till the object reference is resolved and returns the resulting value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_value = ray.get(object_ref_future) \n",
    "object_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a more detailed view of the parallel execution\n",
    "\n",
    "\n",
    "<img src=\"parallel_1000_feet.png\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the ray state client to verify the above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We re-declare the `expensive_computation` but give it a unique name so we can easily track its state and a longer sleep time so we can see the state evolve more clearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "task_sleep_time = 20\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def my_task():\n",
    "    import time\n",
    "\n",
    "    time.sleep(task_sleep_time)\n",
    "    return 1\n",
    "\n",
    "\n",
    "id_ = str(uuid4())[:8]\n",
    "name = f\"expensive_computation_{id_}\"\n",
    "ray_task = my_task.options(name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We submit the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectRef(1e8ff6d236132784ffffffffffffffffffffffff0100000001000000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray_task.remote()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now request the cluster state to see our task running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task expensive_computation_c18f072b is in state=RUNNING running on worker aecd9247 as part of Job ID 01000000\n",
      "task expensive_computation_c18f072b is in state=RUNNING running on worker aecd9247 as part of Job ID 01000000\n",
      "task expensive_computation_c18f072b is in state=RUNNING running on worker aecd9247 as part of Job ID 01000000\n",
      "task expensive_computation_c18f072b is in state=RUNNING running on worker aecd9247 as part of Job ID 01000000\n"
     ]
    }
   ],
   "source": [
    "from ray.util.state import list_tasks\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "while time.time() - start_time < task_sleep_time:\n",
    "    time.sleep(5)\n",
    "    task = next(task for task in list_tasks() if task.name == name)\n",
    "    print(\n",
    "        f\"task {task.name} is in state={task.state} running on worker {task.worker_id[:8]} as part of Job ID {task.job_id}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 100 feet view\n",
    "\n",
    "Let's further detail the lifecycle of a ray task.\n",
    "\n",
    "More specifically here is what a cluster looks like:\n",
    "\n",
    "\n",
    "<img src=\"ray_cluster.png\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to keep in mind:\n",
    "\n",
    "- The head node is a special node that runs the driver and the global control service. \n",
    "- The head node can also spawn worker processes to execute tasks\n",
    "- The Global control service keeps track of cluster state that is not supposed to change often\n",
    "- Each worker process will keep track of all the task it executes and submits in its ownership table\n",
    "- Small objects (< 100KB) are stored in the in-process object store of a worker\n",
    "- Large objects are stored in the plasma store which is shared across worker processes on the same node\n",
    "- plasma store by default is in-memory and takes up 30% of the memory of the node\n",
    "- if plasma store is full, objects are spilled to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the cluster architecture in mind, let's look at the lifecycle of a task in more detail.\n",
    "\n",
    "#### Submitting a task\n",
    "<img src=\"submit_task.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On data locality**:\n",
    "\n",
    "- The owner will select the raylet where most of the objects the task depends on are located\n",
    "  - this can be a raylet running on a different worker node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scheduling a Task\n",
    "\n",
    "<img src=\"schedule_task.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: \n",
    "- Step 0 is \"raylet scheduler finding a node to schedule the task on\"\n",
    "- Step 0 is not shown in the diagram but instead will be discussed in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scheduling deep-dive\n",
    "\n",
    "How does a raylet's scheduler choose a worker node to lease work from?\n",
    "\n",
    "### Classifying nodes as feasible/infeasible and available/unavailable\n",
    "\n",
    "<img src=\"availability_feasibility.png\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scheduling Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Default Hybrid policy\n",
    "\n",
    "\n",
    "This is the default policy used by ray. It is a hybrid policy that combines the following two modes:\n",
    "- Bin packing mode\n",
    "- Load balancing mode\n",
    "  \n",
    "\n",
    "The diagram below shows the two modes in action when scheduling two tasks Task1 and Task2\n",
    "\n",
    "<img src=\"default_hybrid_policy_new.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node Affinity Policy \n",
    "\n",
    "Assigns tasks to a given node in either a strict or soft manner.\n",
    "\n",
    "<img src=\"node_affinity_policy.png\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SPREAD Policy \n",
    "\n",
    "As the name suggests, the SPREAD policy spreads the tasks across all the available nodes.\n",
    "\n",
    "<img src=\"spread_policy_.png\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placement Group Policy\n",
    "\n",
    "In cases when we want to treat a set of nodes as a single unit, we can use placement groups.\n",
    "\n",
    "<img src=\"placement_group_policy.png\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Things to keep in mind**:\n",
    "\n",
    "- A **placement group** is formed from a set of **resource bundles**\n",
    "  - A **resource bundle** is a list of resource requirements that fit in a single node\n",
    "- A **placement group** can specify a **placement strategy** that determines how the **resource bundles** are placed\n",
    "  - The **placement strategy** can be one of the following:\n",
    "    - **PACK**: pack the **resource bundles** into as few nodes as possible\n",
    "    - **SPREAD**: spread the **resource bundles** across as many nodes as possible\n",
    "    - **STRICT_PACK**: pack the **resource bundles** into as few nodes as possible and fail if not possible\n",
    "    - **STRICT_SPREAD**: spread the **resource bundles** across as many nodes as possible and fail if not possible\n",
    "- **Placement Groups** are **atomic** \n",
    "  -  i.e. either all the **resource bundles** are placed or none are placed\n",
    "  -  GCS uses a two-phase commit protocol to ensure atomicity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetching task results\n",
    "\n",
    "<img src=\"fetch_result.png\">\n",
    "\n",
    "Note: If the owner is fetching the result from a different node than the one where the task was executed, the result is first copied to the local object store of the owner node and then returned to the owner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object management and dependency resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drill down on how a task's dependencies are resolved - using the following example of simple batch inference:\n",
    "\n",
    "- we load a model\n",
    "- we use the model to make predictions on an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_model(size_mb):\n",
    "    weights = np.ones((1024, 1024, size_mb), dtype=np.uint8)\n",
    "    assert weights.nbytes / 1024**2 == size_mb\n",
    "    return weights\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def predict(model, input):\n",
    "    return model * input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with this simple implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load 1 GB model in memory\n",
    "model = load_model(1_000) \n",
    "\n",
    "# submit 3 tasks to the cluster\n",
    "futures = ray.get([predict.remote(model, i) for i in range(3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 `predict` tasks that will be submitted.\n",
    "\n",
    "- The owner of each task will need to go over all the task arguments and:\n",
    "    - check that all the arguments are available\n",
    "    - store a reference to all the available arguments in the (plasma) shared or inprocess object store\n",
    "- In the case of our 1 GB model, the owner will make use of the shared object store given our object is > 1KB\n",
    "- Each owner will create a copy of the model and produce an object reference to use as the argument for the task\n",
    "- Each owner process will now execute their task\n",
    "\n",
    "The outcome is that we have made 3 copies of the model in the shared object store.\n",
    "\n",
    "Instead to save on memory, we should use the `ray.put` API to store the model in the shared object store and pass the reference to the model as an argument to the task.\n",
    "\n",
    "Here is the optimized implementation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the model in the object store and get a reference to it\n",
    "model_ref = ray.put(model)\n",
    "\n",
    "# submit 3 tasks to the cluster using the same model reference\n",
    "futures = ray.get([predict.remote(model_ref, i) for i in range(3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 feet view of ray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting debug logs\n",
    "\n",
    "Given the below code, we can inspect the debug logs to see what is happening under the hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 12:19:07,653\tINFO worker.py:1633 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_model(size_mb):\n",
    "    weights = np.ones((1024, 1024, size_mb), dtype=np.uint8)\n",
    "    assert weights.nbytes / 1024**2 == size_mb\n",
    "    return weights\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def predict(model, input):\n",
    "    return model * input\n",
    "\n",
    "\n",
    "model = load_model(size_mb=1000)\n",
    "obj_ref = predict.remote(model, 1)\n",
    "result = ray.get(obj_ref)  # c8ef45ccd0112571ffffffffffffffffffffffff0100000001000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the debug logs, color-categorized and annotated\n",
    "<img src=\"debug_logs_annotated_cropped.png\" height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lifecycle of an Actor\n",
    "\n",
    "An actor is a stateful object that can be used to encapsulate state and methods that operate on that state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why use an actor ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can't naively share a global variable across tasks\n",
    "  - Global variables are not shared across worker processes - i.e. across tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "global_var = 3\n",
    "\n",
    "@ray.remote\n",
    "def increment_global_var():\n",
    "    global global_var\n",
    "    global_var += 1\n",
    "    return global_var\n",
    "\n",
    "@ray.remote\n",
    "def decrement_global_var():\n",
    "    global global_var\n",
    "    global_var -= 1\n",
    "    return global_var\n",
    "\n",
    "step1 = ray.get(increment_global_var.remote())\n",
    "step2 = ray.get(decrement_global_var.remote())\n",
    "\n",
    "# we expect 4, 3 but we get 4, 2\n",
    "# given the two tasks have separate copies of the global variable\n",
    "print(step1, step2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Storing state in a database is slow\n",
    "  - Actors are in-memory\n",
    "  - Actors are distributed across nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 3\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import ray\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def read_from_db(key):\n",
    "    # only for demo purposes: mimic reading from a database\n",
    "    time.sleep(1)\n",
    "    return json.loads(Path(\"table.json\").read_text())[key]\n",
    "\n",
    "\n",
    "def write_to_db(key, val):\n",
    "    data = {key: val}\n",
    "    # only for demo purposes: mimic reading from a database\n",
    "    time.sleep(1)\n",
    "    Path(\"table.json\").write_text(json.dumps(data))\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def increment_global_var():\n",
    "    global_var = read_from_db(\"global_var\")\n",
    "    global_var += 1\n",
    "    write_to_db(\"global_var\", global_var)\n",
    "    return global_var\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def decrement_global_var():\n",
    "    global_var = read_from_db(\"global_var\")\n",
    "    global_var -= 1\n",
    "    write_to_db(\"global_var\", global_var)\n",
    "    return global_var\n",
    "\n",
    "\n",
    "write_to_db(\"global_var\", 3)\n",
    "step1 = ray.get(increment_global_var.remote())\n",
    "step2 = ray.get(decrement_global_var.remote())\n",
    "print(step1, step2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10,000 feet view\n",
    "\n",
    "Let's take an example of a simple counter actor. We create an actor handle by calling `Counter.remote()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "class MyCounter:\n",
    "    def __init__(self) -> None:\n",
    "        self.counter = 0\n",
    "\n",
    "    def increment(self):\n",
    "        time.sleep(3)\n",
    "        self.counter += 1\n",
    "\n",
    "    def get_counter(self):\n",
    "        return self.counter\n",
    "\n",
    "\n",
    "my_counter_handle = MyCounter.remote()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then call methods on the actor handle to increment the counter and get the current value of the counter. The methods will be executed sequentially against the actor process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this will take 3 seconds * 2 = 6 seconds at least\n",
    "ray.get([my_counter_handle.increment.remote() for _ in range(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.get(my_counter_handle.get_counter.remote())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a diagram showing the lifecycle of our actor (note that our actor is referred to as a \"synchronous\" actor)\n",
    "\n",
    "\n",
    "<img src=\"actor_simple_.jpeg\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A special \"create actor\" task is executed on the cluster to create the actor process\n",
    "- The actor process can be thought of as a special worker process\n",
    "- The actor tasks are executed sequentially on the actor process using a FIFO queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1,000 feet view of ray actors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will detail the lifecycle of an actor in more detail.\n",
    "\n",
    "- Actors are always owned by the GCS (global control service), unlike tasks which are owned by the worker process that submitted them\n",
    "- The GCS maintains an actor table that keeps track of all the actors in the cluster\n",
    "- Actors hold the resources they need to execute their tasks until they are killed\n",
    "- Actors can be launched in a detached mode, in which case they do not fate share with a ray driver/job - instead they need to be killed manually\n",
    "\n",
    "See the below diagram for more details\n",
    "\n",
    "\n",
    "<img src=\"actor_centralized.jpeg\" height=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our actors can be asynchronous - this is especially useful for actors whose methods are IO bound and whose state can be easily shared and locked if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from asyncio import sleep\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "class MyAsyncService:\n",
    "    def __init__(self) -> None:\n",
    "        self.fixed_state = 1\n",
    "\n",
    "    async def run(self):\n",
    "        await sleep(15)\n",
    "        return self.fixed_state\n",
    "\n",
    "\n",
    "my_async_actor_handle = MyAsyncService.remote()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the service run is mostly IO bound (sleeping), we can run it asynchronously using an asynchronous actor implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13 ms, sys: 12.9 ms, total: 25.8 ms\n",
      "Wall time: 15.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 1]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ray.get([my_async_actor_handle.run.remote() for _ in range(2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a diagram visualizing task execution against an asynchroneous actor.\n",
    "\n",
    "<img src=\"actor_async.jpeg\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fault tolerance of Ray Actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
