{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"logo-ray.png\" width=\"80px\">\n",
    "\n",
    "\n",
    "# Introduction to Ray Core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lifecycle of a task \n",
    "\n",
    "We start out detailing the full lifecylce of a **ray task** from when it is **created** and submitted till when it is **completed** and the **resulting objects are returned** to the user. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10,000 feet view\n",
    "\n",
    "We have a python function convenitenly named `expensive_computation` which executes an expensive computation. To keep it simple all it does is perform a naive matrix multiplication and returns the number of elements in the resulting matrix. \n",
    "\n",
    "\n",
    "It gets called in sequence a number of times (`n_runs`) to be specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils.py\n",
    "from itertools import product\n",
    "\n",
    "def perform_naive_matrix_multiplication(n):\n",
    "    matrix1 = matrix2 = [[1 for _ in range(n)] for _ in range(n)]\n",
    "\n",
    "    result = [[0 for _ in range(n)] for _ in range(n)]\n",
    "    for i, j, k in product(range(n), range(n), range(n)):\n",
    "        result[i][j] += matrix1[i][k] * matrix2[k][j]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import perform_naive_matrix_multiplication\n",
    "\n",
    "n_runs = 10\n",
    "n = 300\n",
    "\n",
    "def expensive_computation(n):\n",
    "    result = perform_naive_matrix_multiplication(n)\n",
    "    n_rows, n_cols =  len(result), len(result[0])\n",
    "    num_elements_in_matrix = n_rows * n_cols\n",
    "    return num_elements_in_matrix\n",
    "\n",
    "results = [expensive_computation(n) for _ in range(n_runs)]\n",
    "assert sum(results) == n_runs * n * n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the execution visualized\n",
    "\n",
    "<img src=\"sequential_simple_.jpeg\" height=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to:\n",
    "- Run the same function but in a distributed fashion - i.e. in parallel on a cluster of machines\n",
    "\n",
    "We do this by following these steps:\n",
    "- Convert the `expensive_computation` function to a ray task by decorating it with `ray.remote`\n",
    "- Submit a task for execution by calling `future = expensive_computation.remote()`\n",
    "- Use the returned `future` object reference to fetch the result of the function by calling `ray.get(future)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 13:06:39,937\tINFO worker.py:1458 -- Connecting to existing Ray cluster at address: 127.0.0.1:6379...\n",
      "2023-11-21 13:06:39,946\tINFO worker.py:1633 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-11-21 13:06:40,224 I 17097 12251738] logging.cc:230: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to -1\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "\n",
    "@ray.remote  # decorator to convert python function to ray task\n",
    "def expensive_computation(n):\n",
    "    result = perform_naive_matrix_multiplication(n)\n",
    "    n_rows, n_cols =  len(result), len(result[0])\n",
    "    num_elements_in_matrix = n_rows * n_cols\n",
    "    return num_elements_in_matrix\n",
    "\n",
    "# submit n_run ray tasks to a ray cluster\n",
    "# and keep a reference to the task futures\n",
    "futures = [expensive_computation.remote(n) for _ in range(n_runs)]\n",
    "\n",
    "# wait for all tasks to complete and get the resulting objects\n",
    "# results are returned in the same order as submitted\n",
    "results = ray.get(futures)\n",
    "\n",
    "# confirm that we got the right result\n",
    "assert sum(results) == n_runs * n * n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what is happening under the hood:\n",
    "\n",
    "<img src=\"parallel_simple.jpeg\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1000 feet view\n",
    "\n",
    "Let's detail the parallel execution of the function a bit more.\n",
    "\n",
    "More specifically:\n",
    "- **ray tasks** are executed on a **ray cluster** as part of a **ray job**\n",
    "- **ray workers** are the processes that execute the tasks\n",
    "- **futures** in ray are called `ObjectRef`s short for **object references**\n",
    "- results are stored as **objects** in an \"**object store**\"\n",
    "- `ray.get()` is used to wait and fetch the **object value** given the **object reference** from the \"**object store**\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a more detailed view of the parallel execution\n",
    "\n",
    "\n",
    "<img src=\"parallel_1000_feet.png\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the ray state client to verify the above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We re-declare the `expensive_computation` but give it a unique name so we can easily track its state and a longer sleep time so we can see the state evolve more clearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "import ray\n",
    "\n",
    "task_sleep_time = 20\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def my_task():\n",
    "    import time\n",
    "\n",
    "    time.sleep(task_sleep_time)\n",
    "    return 1\n",
    "\n",
    "\n",
    "id_ = str(uuid4())[:8]\n",
    "name = f\"expensive_computation_{id_}\"\n",
    "ray_task = my_task.options(name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We submit the task and inspect the future object reference - we see that it is a ray.ObjectRef with a given id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectRef(35260dd7c82a1dceffffffffffffffffffffffff0a00000001000000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future_object_ref = ray_task.remote()\n",
    "future_object_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now request the cluster state to see our task running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task expensive_computation_ce2fa6ca is in state=RUNNING running on worker 2550baa4 as part of Job ID 0a000000\n",
      "task expensive_computation_ce2fa6ca is in state=RUNNING running on worker 2550baa4 as part of Job ID 0a000000\n",
      "task expensive_computation_ce2fa6ca is in state=RUNNING running on worker 2550baa4 as part of Job ID 0a000000\n",
      "task expensive_computation_ce2fa6ca is in state=RUNNING running on worker 2550baa4 as part of Job ID 0a000000\n",
      "task expensive_computation_ce2fa6ca is in state=FINISHED running on worker 2550baa4 as part of Job ID 0a000000\n",
      "task expensive_computation_ce2fa6ca is in state=FINISHED running on worker 2550baa4 as part of Job ID 0a000000\n"
     ]
    }
   ],
   "source": [
    "from ray.util.state import get_task\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "while time.time() - start_time < (task_sleep_time + 10):\n",
    "    time.sleep(5)\n",
    "    task = get_task(id=future_object_ref.task_id().hex())\n",
    "    print(\n",
    "        f\"task {task.name} is in state={task.state} running on worker {task.worker_id[:8]} as part of Job ID {task.job_id}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `ray.get` to fetch the resulting object value now that the task is completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_value = ray.get(future_object_ref)\n",
    "object_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 100 feet view\n",
    "\n",
    "Let's further detail the lifecycle of a ray task.\n",
    "\n",
    "More specifically here is what a cluster looks like:\n",
    "\n",
    "\n",
    "<img src=\"ray_cluster.png\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to keep in mind:\n",
    "\n",
    "- The **head node** is a special node that runs the **global control service**, **cluster level services** and usually the **driver**\n",
    "  - The **global control service** keeps track of the **cluster state** that is not supposed to change often\n",
    "  - Cluster level services are services that are shared across the cluster suc as autoscaling, job submission, etc. \n",
    "  - The **driver** can submit tasks but does not execute them \n",
    "- Each **worker process** will keep track of all the **tasks** it owns/submits in its **ownership table**\n",
    "- Small **objects** (< 100KB) are stored in the **in-process object store** of a **worker**\n",
    "- Large **objects** are stored in the **plasma object store** which is **shared across worker processes** on the same node\n",
    "  - The **plasma object store** by default is in-memory and takes up **30% of the memory of the node**\n",
    "  - If the **plasma object store** is full, objects are **spilled to disk**\n",
    "  - The **plasma object store** is also referred to as the **shared memory object store**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the cluster architecture in mind, let's look at the lifecycle of a task in more detail.\n",
    "\n",
    "#### Submitting a task\n",
    "<img src=\"submit_task.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data locality in ray\n",
    "\n",
    "- The owner will select the **raylet** where **most of the objects the task depends on** are located\n",
    "  - This can be a **raylet** running on a **different node**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scheduling a Task\n",
    "\n",
    "<img src=\"scheduling_task.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scheduling policies deep-dive\n",
    "\n",
    "How does a raylet's scheduler choose a worker node to lease work from?\n",
    "\n",
    "### Classifying nodes as feasible/infeasible and available/unavailable\n",
    "\n",
    "<img src=\"resource_state_definition.png\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that every 100ms, the **GCS pulls resource availability** from each **raylet** and then aggregates and **rebroadcasts them back to each raylet**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scheduling Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Default Hybrid policy\n",
    "\n",
    "\n",
    "This is the default policy used by ray. It is a hybrid policy that combines the following two modes:\n",
    "- Bin packing mode\n",
    "- Load balancing mode\n",
    "  \n",
    "\n",
    "The diagram below shows the two modes in action when scheduling two tasks Task1 and Task2\n",
    "\n",
    "<img src=\"default_hybrid_policy_.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** you can set the following environment variables to configure the default hybrid policy:\n",
    "\n",
    "- `RAY_scheduler_spread_threshold` - default is 0.5 or 50% utilization of the node\n",
    "- `RAY_scheduler_top_k_fraction` - default is 0.2 or 20% of the nodes\n",
    "  - You can also set `RAY_scheduler_top_k_absolute` to set an absolute number of nodes to use\n",
    "  - Note that it is the max of `RAY_scheduler_top_k_fraction` and `RAY_scheduler_top_k_absolute` that is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "@ray.remote(scheduling_strategy=\"DEFAULT\") # this is the default so we don't need to specify it\n",
    "def default_schedule_func():\n",
    "    return 2\n",
    "\n",
    "ray.get(default_schedule_func.remote())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node Affinity Policy \n",
    "\n",
    "Assigns tasks to a given node in either a strict or soft manner.\n",
    "\n",
    "<img src=\"node_affinity_policy.png\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "from ray.util.scheduling_strategies import NodeAffinitySchedulingStrategy\n",
    "\n",
    "\n",
    "@ray.remote(\n",
    "    scheduling_strategy=NodeAffinitySchedulingStrategy(\n",
    "        node_id=ray.get_runtime_context().get_node_id(),\n",
    "        soft=False,\n",
    "    )\n",
    ")\n",
    "def node_affinity_schedule():\n",
    "    return 2\n",
    "\n",
    "\n",
    "ray.get(node_affinity_schedule.remote())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SPREAD Policy \n",
    "\n",
    "As the name suggests, the SPREAD policy spreads the tasks across the nodes.\n",
    "\n",
    "Note that it spreads across all the available nodes first and then the feasible nodes.\n",
    "\n",
    "<img src=\"spread_scheduling_policy.png\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "@ray.remote(scheduling_strategy=\"SPREAD\")\n",
    "def spread_default_func():\n",
    "    return 2\n",
    "\n",
    "ray.get(spread_default_func.remote())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placement Group Policy\n",
    "\n",
    "In cases when we want to treat a set of resources as a single unit, we can use placement groups.\n",
    "\n",
    "\n",
    "<img src=\"placement_group_policy.png\" width=\"300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Things to keep in mind**:\n",
    "\n",
    "- A **placement group** is formed from a set of **resource bundles**\n",
    "  - A **resource bundle** is a list of resource requirements that fit in a single node\n",
    "- A **placement group** can specify a **placement strategy** that determines how the **resource bundles** are placed\n",
    "  - The **placement strategy** can be one of the following:\n",
    "    - **PACK**: pack the **resource bundles** into as few nodes as possible\n",
    "    - **SPREAD**: spread the **resource bundles** across as many nodes as possible\n",
    "    - **STRICT_PACK**: pack the **resource bundles** into as few nodes as possible and fail if not possible\n",
    "    - **STRICT_SPREAD**: spread the **resource bundles** across as many nodes as possible and fail if not possible\n",
    "- **Placement Groups** are **atomic** \n",
    "  -  i.e. either all the **resource bundles** are placed or none are placed\n",
    "  -  GCS uses a two-phase commit protocol to ensure atomicity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'placement_group_id': 'b54ea3ae05ba27635687307740520a000000', 'name': 'my_pg', 'bundles': {0: {'CPU': 0.1}}, 'bundles_to_node_id': {0: '437a139387ca944e4b08ec1c3bb45382e2dc2d6274c0243a8121d760'}, 'strategy': 'PACK', 'state': 'CREATED', 'stats': {'end_to_end_creation_latency_ms': 5.396, 'scheduling_latency_ms': 4.704, 'scheduling_attempt': 1, 'highest_retry_delay_ms': 0.0, 'scheduling_state': 'FINISHED'}}\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.util.scheduling_strategies import PlacementGroupSchedulingStrategy\n",
    "# Import placement group related functions\n",
    "from ray.util.placement_group import (\n",
    "    placement_group,\n",
    "    placement_group_table,\n",
    "    remove_placement_group,\n",
    ")\n",
    "\n",
    "# Reserve a placement group of 1 bundle that reserves 0.1 CPU\n",
    "pg = placement_group([{\"CPU\": 0.1}], strategy=\"PACK\", name=\"my_pg\")\n",
    "\n",
    "# Wait until placement group is created.\n",
    "ray.get(pg.ready(), timeout=10)\n",
    "\n",
    "# look at placement group states using the table\n",
    "print(placement_group_table(pg))\n",
    "\n",
    "\n",
    "@ray.remote(\n",
    "    scheduling_strategy=PlacementGroupSchedulingStrategy(\n",
    "        placement_group=pg,\n",
    "    ),\n",
    "    # task requirement needs to be less than placement group capacity\n",
    "    num_cpus=0.1,\n",
    ")\n",
    "def placement_group_schedule():\n",
    "    return 2\n",
    "\n",
    "\n",
    "out = ray.get(placement_group_schedule.remote())\n",
    "print(out)\n",
    "\n",
    "# Remove placement group.\n",
    "remove_placement_group(pg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetching task results\n",
    "\n",
    "<img src=\"fetch_result.png\">\n",
    "\n",
    "Note: If the owner is fetching the result from a different node than the one where the task was executed, the result is first copied to the local object store of the owner node and then returned to the owner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object management and dependency resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drill down on how a task's dependencies are resolved - using the following example of simple batch inference:\n",
    "\n",
    "- we load a model\n",
    "- we use the model to make predictions on an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_model(size_mb):\n",
    "    weights = np.ones((1024, 1024, size_mb), dtype=np.uint8)\n",
    "    assert weights.nbytes / 1024**2 == size_mb\n",
    "    return weights\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def predict(model, input):\n",
    "    return model * input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with this simple implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load 1 GB model in memory\n",
    "model = load_model(1_000) \n",
    "\n",
    "# submit 3 tasks to the cluster\n",
    "futures = ray.get([predict.remote(model, i) for i in range(3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 `predict` tasks that will be submitted.\n",
    "\n",
    "- The owner of each task will need to go over all the task arguments and:\n",
    "    - check that all the arguments are available\n",
    "    - store a reference to all the available arguments in the plasma/shared object store or inprocess object store\n",
    "- In the case of our 1 GB \"model\", the owner will make use of the shared object store given it exceeds the 100KB limit of the inprocess object store\n",
    "- Each owner will create a copy of the model and produce an object reference to use as the argument for the task\n",
    "- Each owner process will now execute their task\n",
    "\n",
    "The outcome is that we have made 3 copies of the model in the shared object store.\n",
    "\n",
    "Instead to save on memory, we should use the `ray.put` API to store the model in the shared object store and pass the reference to the model as an argument to the task.\n",
    "\n",
    "Here is the optimized implementation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the model in the object store and get a reference to it\n",
    "model_ref = ray.put(model)\n",
    "\n",
    "# submit 3 tasks to the cluster using the same model reference\n",
    "futures = ray.get([predict.remote(model_ref, i) for i in range(3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 feet view of ray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting debug logs\n",
    "\n",
    "Given the below code, we can inspect the debug logs to see what is happening under the hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_model(size_mb):\n",
    "    weights = np.ones((1024, 1024, size_mb), dtype=np.uint8)\n",
    "    assert weights.nbytes / 1024**2 == size_mb\n",
    "    return weights\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def predict(model, input):\n",
    "    return model * input\n",
    "\n",
    "\n",
    "model = load_model(size_mb=1000)\n",
    "obj_ref = predict.remote(model, 1)\n",
    "result = ray.get(obj_ref)  # c8ef45ccd0112571ffffffffffffffffffffffff0100000001000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the debug logs, color-categorized and annotated\n",
    "\n",
    "<img src=\"debug_logs_annotated_cropped.png\" height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fault Tolerance of Ray Tasks and Objects\n",
    "\n",
    "- If a task raises an application-level exception, the task will fail and the exception will be propagated to the caller.\n",
    "- If instead a system-level failures, i.e the worker process executing the task crashes then:\n",
    "    - Ray will rerun the task until either the task succeeds or the maximum number of retries is exceeded. \n",
    "        - The default number of retries is 3 and can be overridden by specifying max_retries in the @ray.remote decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "application-level exceptions shortcircuit retries\n"
     ]
    }
   ],
   "source": [
    "# application-level failure flakiness but with infinite retries\n",
    "import sys\n",
    "import ray\n",
    "import pickle\n",
    "\n",
    "with open(\"x.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\"x\": 0}, f)    \n",
    "\n",
    "@ray.remote(max_retries=-1) # infinite retries\n",
    "def flaky_app_task():\n",
    "    with open(\"x.pkl\", \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    x = data[\"x\"]\n",
    "    if x % 2 == 0:\n",
    "        x += 1\n",
    "        with open(\"x.pkl\", \"wb\") as f:\n",
    "            data = pickle.dump({\"x\": x}, f)\n",
    "        raise ValueError(\"x is even - that's odd!\")\n",
    "    return 1\n",
    "\n",
    "try:\n",
    "    out = ray.get(flaky_app_task.remote())\n",
    "except ray.exceptions.RayTaskError:\n",
    "    print(\"application-level exceptions shortcircuit retries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 13:07:53,095\tWARNING worker.py:2058 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 9d02545c60337ae57994a803c9957ee8de611a840a000000 Worker ID: 2550baa4eb6e9ea955ce526c57c1a7c298da9298b7888c8a7fbe96dc Node ID: 437a139387ca944e4b08ec1c3bb45382e2dc2d6274c0243a8121d760 Worker IP address: 127.0.0.1 Worker port: 10199 Worker PID: 17097 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned.\n",
      "\u001b[2m\u001b[36m(flaky_sys_task pid=17097)\u001b[0m Worker exits with an exit code 1.\n",
      "\u001b[2m\u001b[36m(flaky_sys_task pid=17097)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(flaky_sys_task pid=17097)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1999, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(flaky_sys_task pid=17097)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1894, in ray._raylet.execute_task_with_cancellation_handler\n",
      "\u001b[2m\u001b[36m(flaky_sys_task pid=17097)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1558, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(flaky_sys_task pid=17097)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1559, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(flaky_sys_task pid=17097)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1610, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(flaky_sys_task pid=17097)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1616, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(flaky_sys_task pid=17097)\u001b[0m   File \"/var/folders/5s/b_0j_yts17zc8wdkwf3nxlmr0000gn/T/ipykernel_15786/2138810642.py\", line 18, in flaky_sys_task\n",
      "\u001b[2m\u001b[36m(flaky_sys_task pid=17097)\u001b[0m SystemExit: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "returned 1 after retrying worker failure\n"
     ]
    }
   ],
   "source": [
    "# system-level failure flakiness but with infinite retries\n",
    "import sys\n",
    "import ray\n",
    "import pickle\n",
    "\n",
    "with open(\"y.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\"y\": 0}, f)    \n",
    "\n",
    "@ray.remote(max_retries=-1)\n",
    "def flaky_sys_task():\n",
    "    with open(\"y.pkl\", \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    y = data[\"y\"]\n",
    "    if y % 2 == 0:\n",
    "        y += 1\n",
    "        with open(\"y.pkl\", \"wb\") as f:\n",
    "            data = pickle.dump({\"y\": y}, f)\n",
    "        raise sys.exit(1)\n",
    "    return 1\n",
    "\n",
    "# never raises an error given retries eventually succeed\n",
    "out = ray.get(flaky_sys_task.remote())\n",
    "print(\"returned\", out, \"after retrying worker failure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below diagram shows the fault tolerance of ray objects - taken from https://www.usenix.org/system/files/nsdi21-wang.pdf\n",
    "\n",
    "<img src=\"object_fault_tolerance.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When an object value is lost from the object store, such as during node failures\n",
    "- Ray will use lineage reconstruction to recover the object.\n",
    "- Ray will first automatically attempt to recover the value by looking for copies of the same object on other nodes.\n",
    "  - If none are found, then Ray will automatically recover the value by re-executing the task that previously created the value. \n",
    "    - Arguments to the task are recursively reconstructed through the same mechanism.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lifecycle of an Actor\n",
    "\n",
    "An actor is a stateful object that can be used to encapsulate state and methods that operate on that state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why use an actor ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can't naively share a global variable across tasks\n",
    "  - Global variables are not shared across worker processes - i.e. across tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "global_var = 3\n",
    "\n",
    "@ray.remote\n",
    "def increment_global_var():\n",
    "    global global_var\n",
    "    global_var += 1\n",
    "    return global_var\n",
    "\n",
    "@ray.remote\n",
    "def decrement_global_var():\n",
    "    global global_var\n",
    "    global_var -= 1\n",
    "    return global_var\n",
    "\n",
    "step1 = ray.get(increment_global_var.remote())\n",
    "step2 = ray.get(decrement_global_var.remote())\n",
    "\n",
    "# we expect 4, 3 but we get 4, 2\n",
    "# given the two tasks have separate copies of the global variable\n",
    "print(step1, step2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Storing state in a database is slow\n",
    "  - Actors are in-memory\n",
    "  - Actors are distributed across nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-11-21 13:07:54,354 I 17280 12252812] logging.cc:230: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 3\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import ray\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def read_from_db(key):\n",
    "    # only for demo purposes: mimic reading from a database\n",
    "    time.sleep(1)\n",
    "    return json.loads(Path(\"table.json\").read_text())[key]\n",
    "\n",
    "\n",
    "def write_to_db(key, val):\n",
    "    data = {key: val}\n",
    "    # only for demo purposes: mimic reading from a database\n",
    "    time.sleep(1)\n",
    "    Path(\"table.json\").write_text(json.dumps(data))\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def increment_global_var():\n",
    "    global_var = read_from_db(\"global_var\")\n",
    "    global_var += 1\n",
    "    write_to_db(\"global_var\", global_var)\n",
    "    return global_var\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def decrement_global_var():\n",
    "    global_var = read_from_db(\"global_var\")\n",
    "    global_var -= 1\n",
    "    write_to_db(\"global_var\", global_var)\n",
    "    return global_var\n",
    "\n",
    "\n",
    "write_to_db(\"global_var\", 3)\n",
    "step1 = ray.get(increment_global_var.remote())\n",
    "step2 = ray.get(decrement_global_var.remote())\n",
    "print(step1, step2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10,000 feet view\n",
    "\n",
    "Let's take an example of a simple counter actor. We create an actor handle by calling `Counter.remote()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "class MyCounter:\n",
    "    def __init__(self) -> None:\n",
    "        self.counter = 0\n",
    "\n",
    "    def increment(self):\n",
    "        time.sleep(3)\n",
    "        self.counter += 1\n",
    "\n",
    "    def get_counter(self):\n",
    "        return self.counter\n",
    "\n",
    "\n",
    "my_counter_handle = MyCounter.remote()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then call methods on the actor handle to increment the counter and get the current value of the counter. The methods will be executed sequentially against the actor process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-11-21 13:07:59,750 I 17292 12252921] logging.cc:230: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to -1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this will take 3 seconds * 2 = 6 seconds at least\n",
    "ray.get([my_counter_handle.increment.remote() for _ in range(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.get(my_counter_handle.get_counter.remote())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a diagram showing the lifecycle of our actor (note that our actor is referred to as a \"synchronous\" actor)\n",
    "\n",
    "\n",
    "<img src=\"actor_simple_.jpeg\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A special \"create actor\" task is executed on the cluster to create the actor process\n",
    "- The actor process can be thought of as a special worker process\n",
    "- The actor tasks are executed sequentially on the actor process using a FIFO queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1,000 feet view of ray actors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will detail the lifecycle of an actor in more detail.\n",
    "\n",
    "- Actors are always owned by the GCS (global control service), unlike tasks which are owned by the worker process that submitted them\n",
    "- The GCS maintains an actor table that keeps track of all the actors in the cluster\n",
    "- Actors hold the resources they need to execute their tasks until they are killed\n",
    "- Actors can be launched in a detached mode, in which case they do not fate share with a ray driver/job - instead they need to be killed manually\n",
    "\n",
    "See the below diagram for more details\n",
    "\n",
    "\n",
    "<img src=\"actor_centralized.jpeg\" height=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asynchronous Actors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our actors can be asynchronous - this is especially useful for actors whose methods are IO bound and whose state can be easily shared and locked if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from asyncio import sleep\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "class MyAsyncService:\n",
    "    def __init__(self) -> None:\n",
    "        self.fixed_state = 1\n",
    "\n",
    "    async def run(self):\n",
    "        await sleep(15)\n",
    "        return self.fixed_state\n",
    "\n",
    "\n",
    "my_async_actor_handle = MyAsyncService.remote()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the service run is mostly IO bound (sleeping), we can run it asynchronously using an asynchronous actor implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 13:08:05,853\tWARNING worker.py:2058 -- WARNING: 10 PYTHON worker processes have been started on node: 437a139387ca944e4b08ec1c3bb45382e2dc2d6274c0243a8121d760 with address: 127.0.0.1. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds).\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-11-21 13:08:06,106 I 17309 12253081] logging.cc:230: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.8 ms, sys: 23.7 ms, total: 50.6 ms\n",
      "Wall time: 15.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 1]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ray.get([my_async_actor_handle.run.remote() for _ in range(2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a diagram visualizing task execution against an asynchroneous actor.\n",
    "\n",
    "<img src=\"actor_async.jpeg\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fault tolerance of Ray Actors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ray can automatically restart actors that crash unexpectedly. \n",
    "  - This behavior is controlled using `max_restarts`, which sets the maximum number of times that an actor will be restarted.   \n",
    "- When an actor is restarted, its state will be recreated by rerunning its constructor. After the specified number of restarts, subsequent actor methods will raise a RayActorError.\n",
    "- Onus is on the user to manually implement ray actor checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 13:08:21,198\tWARNING worker.py:2058 -- WARNING: 12 PYTHON worker processes have been started on node: 437a139387ca944e4b08ec1c3bb45382e2dc2d6274c0243a8121d760 with address: 127.0.0.1. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds).\n",
      "2023-11-21 13:08:21,467\tWARNING worker.py:2058 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff3c83735e203fae064224a5a90a000000 Worker ID: 668157d37f5f80dbe3b81dfd3110f157bb6823d6e8b5266dbdd9f9bc Node ID: 437a139387ca944e4b08ec1c3bb45382e2dc2d6274c0243a8121d760 Worker IP address: 127.0.0.1 Worker port: 10203 Worker PID: 18583 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-11-21 13:08:21,435 I 18583 12257661] logging.cc:230: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to -1\n",
      "\u001b[2m\u001b[36m(ImmortalActor pid=18583)\u001b[0m Worker exits with an exit code 1.\n",
      "\u001b[2m\u001b[36m(ImmortalActor pid=18583)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(ImmortalActor pid=18583)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1999, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(ImmortalActor pid=18583)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1894, in ray._raylet.execute_task_with_cancellation_handler\n",
      "\u001b[2m\u001b[36m(ImmortalActor pid=18583)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1558, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(ImmortalActor pid=18583)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1559, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(ImmortalActor pid=18583)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1610, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(ImmortalActor pid=18583)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1616, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(ImmortalActor pid=18583)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1556, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(ImmortalActor pid=18583)\u001b[0m   File \"/Users/marwan/.pyenv/versions/3.9.6/envs/ray-core-deep-dive/lib/python3.9/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(ImmortalActor pid=18583)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(ImmortalActor pid=18583)\u001b[0m   File \"/Users/marwan/.pyenv/versions/3.9.6/envs/ray-core-deep-dive/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 467, in _resume_span\n",
      "\u001b[2m\u001b[36m(ImmortalActor pid=18583)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(ImmortalActor pid=18583)\u001b[0m   File \"/var/folders/5s/b_0j_yts17zc8wdkwf3nxlmr0000gn/T/ipykernel_15786/882494078.py\", line 25, in update\n",
      "\u001b[2m\u001b[36m(ImmortalActor pid=18583)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-11-21 13:08:21,711 I 18584 12257689] logging.cc:230: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to -1\n",
      "2023-11-21 13:08:22,054\tWARNING worker.py:2058 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff3c83735e203fae064224a5a90a000000 Worker ID: 162d6288dde1dc5960b1176d2e6969fa9bab267f72caba6ca73cefb3 Node ID: 437a139387ca944e4b08ec1c3bb45382e2dc2d6274c0243a8121d760 Worker IP address: 127.0.0.1 Worker port: 10204 Worker PID: 18584 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-11-21 13:08:22,357 I 18591 12257749] logging.cc:230: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to -1\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import os\n",
    "import json\n",
    "import tempfile\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "\n",
    "@ray.remote(max_restarts=-1, max_task_retries=-1)\n",
    "class ImmortalActor:\n",
    "    def __init__(self, checkpoint_file):\n",
    "        self.checkpoint_file = checkpoint_file\n",
    "\n",
    "        if os.path.exists(self.checkpoint_file):\n",
    "            # Restore from a checkpoint\n",
    "            with open(self.checkpoint_file, \"r\") as f:\n",
    "                self.state = json.load(f)\n",
    "        else:\n",
    "            self.state = {}\n",
    "\n",
    "    def update(self, key, value):\n",
    "        import random\n",
    "\n",
    "        if random.randrange(10) < 5:\n",
    "            sys.exit(1)\n",
    "\n",
    "        self.state[key] = value\n",
    "\n",
    "        # Checkpoint the latest state\n",
    "        with open(self.checkpoint_file, \"w\") as f:\n",
    "            json.dump(self.state, f)\n",
    "\n",
    "    def get(self, key):\n",
    "        return self.state[key]\n",
    "\n",
    "\n",
    "checkpoint_dir = tempfile.mkdtemp()\n",
    "actor = ImmortalActor.remote(os.path.join(checkpoint_dir, \"checkpoint.json\"))\n",
    "ray.get(actor.update.remote(\"1\", 1))\n",
    "ray.get(actor.update.remote(\"2\", 2))\n",
    "assert ray.get(actor.get.remote(\"1\")) == 1\n",
    "shutil.rmtree(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
