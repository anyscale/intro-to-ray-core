{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"logo-ray.png\" width=\"80px\">\n",
    "\n",
    "\n",
    "# Introduction to Ray Core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lifecycle of a task \n",
    "\n",
    "We start out detailing the full lifecylce of a **ray task** from when it is **created** and submitted till when it is **completed** and the **resulting objects are returned** to the user. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10,000 feet view\n",
    "\n",
    "We have a python function convenitenly named `expensive_computation` which executes an expensive computation. To keep it simple all it does is perform a naive matrix multiplication and returns the number of elements in the resulting matrix. \n",
    "\n",
    "\n",
    "It gets called in sequence a number of times (`n_runs`) to be specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils.py\n",
    "from itertools import product\n",
    "\n",
    "def perform_naive_matrix_multiplication(n):\n",
    "    matrix1 = matrix2 = [[1 for _ in range(n)] for _ in range(n)]\n",
    "\n",
    "    result = [[0 for _ in range(n)] for _ in range(n)]\n",
    "    for i, j, k in product(range(n), range(n), range(n)):\n",
    "        result[i][j] += matrix1[i][k] * matrix2[k][j]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import perform_naive_matrix_multiplication\n",
    "\n",
    "n_runs = 10\n",
    "n = 300\n",
    "\n",
    "def expensive_computation(n):\n",
    "    result = perform_naive_matrix_multiplication(n)\n",
    "    n_rows, n_cols = len(result), len(result[0])\n",
    "    num_elements_in_matrix = n_rows * n_cols\n",
    "    return num_elements_in_matrix\n",
    "\n",
    "\n",
    "results = [expensive_computation(n) for _ in range(n_runs)]\n",
    "assert sum(results) == n_runs * n * n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the execution visualized\n",
    "\n",
    "<img src=\"sequential_simple_.jpeg\" height=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to:\n",
    "- Run the same function but in a distributed fashion - i.e. in parallel on a cluster of machines\n",
    "\n",
    "We do this by following these steps:\n",
    "- Convert the `expensive_computation` function to a ray task by decorating it with `ray.remote`\n",
    "- Submit a task for execution by calling `future = expensive_computation.remote()`\n",
    "- Use the returned `future` object reference to fetch the result of the function by calling `ray.get(future)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 22:42:12,764\tINFO worker.py:1633 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "\n",
    "@ray.remote  # decorator to convert python function to ray task\n",
    "def expensive_computation(n):\n",
    "    result = perform_naive_matrix_multiplication(n)\n",
    "    n_rows, n_cols = len(result), len(result[0])\n",
    "    num_elements_in_matrix = n_rows * n_cols\n",
    "    return num_elements_in_matrix\n",
    "\n",
    "\n",
    "# submit n_run ray tasks to a ray cluster\n",
    "# and keep a reference to the task futures\n",
    "futures = [expensive_computation.remote(n) for _ in range(n_runs)]\n",
    "\n",
    "# wait for all tasks to complete and get the resulting objects\n",
    "# results are returned in the same order as submitted\n",
    "results = ray.get(futures)\n",
    "\n",
    "# confirm that we got the right result\n",
    "assert sum(results) == n_runs * n * n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what is happening under the hood:\n",
    "\n",
    "<img src=\"parallel_simple.jpeg\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1000 feet view\n",
    "\n",
    "Let's detail the parallel execution of the function a bit more.\n",
    "\n",
    "More specifically:\n",
    "- **ray tasks** are executed on a **ray cluster** as part of a **ray job**\n",
    "   - You can think of a **ray job** as the collection of tasks, objects, and actors originating from the same runtime environment\n",
    "- **ray worker processes** are the processes that execute the tasks\n",
    "- **futures** in ray are called `ObjectRef`s short for **object references**\n",
    "- results are stored as **objects** in an \"**object store**\"\n",
    "- `ray.get()` is used to wait and fetch the **object value** given the **object reference** from the \"**object store**\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a more detailed view of the parallel execution\n",
    "\n",
    "\n",
    "<img src=\"parallel_execution_1000ft.png\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the ray state client to verify the above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We re-declare the `expensive_computation` but give it a unique name so we can easily track its state and a longer sleep time so we can see the state evolve more clearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "import ray\n",
    "\n",
    "task_sleep_time = 20\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def my_task():\n",
    "    import time\n",
    "\n",
    "    time.sleep(task_sleep_time)\n",
    "    return 1\n",
    "\n",
    "\n",
    "id_ = str(uuid4())[:8]\n",
    "name = f\"expensive_computation_{id_}\"\n",
    "ray_task = my_task.options(name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We submit the task and inspect the future object reference - we see that it is a ray.ObjectRef with a given id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_object_ref = ray_task.remote()\n",
    "future_object_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now request the cluster state to see our task running and transitioning through some of its states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.util.state import get_task\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "while time.time() - start_time < (task_sleep_time + 10):\n",
    "    time.sleep(5)\n",
    "    task = get_task(id=future_object_ref.task_id().hex())\n",
    "    print(\n",
    "        f\"task {task.name} is in state={task.state} running on worker {task.worker_id[:8]} as part of Job ID {task.job_id}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general this diagram below shows the high-level state transitions a task will go through on it happy path\n",
    "\n",
    "<img src=\"state_transition_simplified.png\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use `ray.get` to fetch the resulting object value now that the task is completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_value = ray.get(future_object_ref)\n",
    "object_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 100 feet view\n",
    "\n",
    "Let's further detail the lifecycle of a ray task.\n",
    "\n",
    "More specifically here is what a cluster looks like:\n",
    "\n",
    "\n",
    "<img src=\"ray_cluster.png\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to keep in mind:\n",
    "\n",
    "- The **head node** is a special node that runs the **global control service**, **cluster level services** and usually the **driver**\n",
    "  - The **global control service** keeps track of the **cluster state** that is not supposed to change often\n",
    "  - Cluster level services are services that are shared across the cluster suc as autoscaling, job submission, etc. \n",
    "  - The **driver** can submit tasks but does not execute them \n",
    "- Each **worker process** will keep track of all the **tasks** it owns/submits in its **ownership table**\n",
    "- Small **objects** (< 100KB) are stored in the **in-process object store** of a **worker**\n",
    "- Large **objects** are stored in the **plasma object store** which is **shared across worker processes** on the same node\n",
    "  - The **plasma object store** by default is in-memory and takes up **30% of the memory of the node**\n",
    "  - If the **plasma object store** is full, objects are **spilled to disk**\n",
    "  - The **plasma object store** is also referred to as the **shared memory object store**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed ownership work in ray?\n",
    "\n",
    "- The worker process that submits a task is the **owner** of that task\n",
    "  - Note that this is not the same as the worker process that executes the task\n",
    "  \n",
    "Here is a sample diagram showing how ownership works\n",
    "<img src=\"ownership_diagram.png\" width=\"500px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Driver submits `a`\n",
    "- This means the **Driver** is the **owner** of `x` (result of putting object in store) and `y` from **task 1**\n",
    "- Then the **worker process** executing **task a** will submit **task b**\n",
    "    - This means the **worker process** executing **task a** is the **owner** of the **resulting object** `z` from **task b**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the cluster architecture in mind, let's look at the lifecycle of a task in more detail.\n",
    "\n",
    "#### Submitting a task\n",
    "<img src=\"submit_task_detailed.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data locality in ray\n",
    "\n",
    "- The owner will select the **raylet** where **bulk of the objects the task depends on** are located\n",
    "  - This can be a **raylet** running on a **different node**!\n",
    "  - Bulk is determined by the dependency's object size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"selecting_raylet.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scheduling a Task\n",
    "\n",
    "<img src=\"schedule_task_.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scheduling policies deep-dive\n",
    "\n",
    "How does a raylet's scheduler choose a worker node to lease work from?\n",
    "\n",
    "### Classifying nodes as feasible/infeasible and available/unavailable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that every 100ms, the **GCS pulls resource availability** from each **raylet** and then aggregates and **rebroadcasts them back to each raylet**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from df_utils import styled_df\n",
    "import pandas as pd\n",
    "\n",
    "# Read DataFrame from CSV\n",
    "df_read = pd.read_csv(\"raylet_node_classification.csv\", header=[0, 1])\n",
    "\n",
    "# Display the read DataFrame\n",
    "styled_df(df_read)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scheduling Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Default Hybrid policy\n",
    "\n",
    "\n",
    "This is the default policy used by ray. It is a hybrid policy that combines the following two heuristics:\n",
    "- Bin packing heuristic\n",
    "- Load balancing heuristic\n",
    "\n",
    "**Make sure to note it is the local node to the chosen raylet**\n",
    "\n",
    "The diagram below shows the two modes in action when scheduling two tasks Task1 and Task2\n",
    "\n",
    "<img src=\"scheduling_hybrid_heuristic.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** you can set the following environment variables to configure the default hybrid policy:\n",
    "\n",
    "- `RAY_scheduler_spread_threshold` - default is 0.5 or 50% utilization of the node\n",
    "- `RAY_scheduler_top_k_fraction` - default is 0.2 or 20% of the nodes\n",
    "  - You can also set `RAY_scheduler_top_k_absolute` to set an absolute number of nodes to use\n",
    "  - Note that it is the max of `RAY_scheduler_top_k_fraction` and `RAY_scheduler_top_k_absolute` that is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "@ray.remote(scheduling_strategy=\"DEFAULT\") # this is the default so we don't need to specify it\n",
    "def default_schedule_func():\n",
    "    return 2\n",
    "\n",
    "ray.get(default_schedule_func.remote())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node Affinity Policy \n",
    "\n",
    "Assigns tasks to a given node in either a strict or soft manner.\n",
    "\n",
    "<img src=\"node_affinity_policy.png\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.util.scheduling_strategies import NodeAffinitySchedulingStrategy\n",
    "\n",
    "\n",
    "@ray.remote(\n",
    "    scheduling_strategy=NodeAffinitySchedulingStrategy(\n",
    "        node_id=ray.get_runtime_context().get_node_id(),\n",
    "        soft=False,\n",
    "    )\n",
    ")\n",
    "def node_affinity_schedule():\n",
    "    return 2\n",
    "\n",
    "\n",
    "ray.get(node_affinity_schedule.remote())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SPREAD Policy \n",
    "\n",
    "As the name suggests, the SPREAD policy spreads the tasks across the nodes.\n",
    "\n",
    "Note that it spreads across all the available nodes first and then the feasible nodes.\n",
    "\n",
    "Behaves like a best-effort round-robin\n",
    "\n",
    "<img src=\"spread_scheduling_policy_.png\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "\n",
    "@ray.remote(scheduling_strategy=\"SPREAD\")\n",
    "def spread_default_func():\n",
    "    return 2\n",
    "\n",
    "\n",
    "ray.get(spread_default_func.remote())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placement Group Policy\n",
    "\n",
    "In cases when we want to treat a set of resources as a single unit, we can use placement groups.\n",
    "\n",
    "\n",
    "<img src=\"placement_group_policy_.png\" width=\"300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Things to keep in mind**:\n",
    "\n",
    "- A **placement group** is formed from a set of **resource bundles**\n",
    "  - A **resource bundle** is a list of resource requirements that fit in a single node\n",
    "- A **placement group** can specify a **placement strategy** that determines how the **resource bundles** are placed\n",
    "  - The **placement strategy** can be one of the following:\n",
    "    - **PACK**: pack the **resource bundles** into as few nodes as possible\n",
    "    - **SPREAD**: spread the **resource bundles** across as many nodes as possible\n",
    "    - **STRICT_PACK**: pack the **resource bundles** into as few nodes as possible and fail if not possible\n",
    "    - **STRICT_SPREAD**: spread the **resource bundles** across as many nodes as possible and fail if not possible\n",
    "- **Placement Groups** are **atomic** \n",
    "  -  i.e. either all the **resource bundles** are placed or none are placed\n",
    "  -  GCS uses a two-phase commit protocol to ensure atomicity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.util.scheduling_strategies import PlacementGroupSchedulingStrategy\n",
    "# Import placement group related functions\n",
    "from ray.util.placement_group import (\n",
    "    placement_group,\n",
    "    placement_group_table,\n",
    "    remove_placement_group,\n",
    ")\n",
    "\n",
    "# Reserve a placement group of 1 bundle that reserves 0.1 CPU\n",
    "pg = placement_group([{\"CPU\": 0.1}], strategy=\"PACK\", name=\"my_pg\")\n",
    "\n",
    "# Wait until placement group is created.\n",
    "ray.get(pg.ready(), timeout=10)\n",
    "\n",
    "# look at placement group states using the table\n",
    "print(placement_group_table(pg))\n",
    "\n",
    "\n",
    "@ray.remote(\n",
    "    scheduling_strategy=PlacementGroupSchedulingStrategy(\n",
    "        placement_group=pg,\n",
    "    ),\n",
    "    # task requirement needs to be less than placement group capacity\n",
    "    num_cpus=0.1,\n",
    ")\n",
    "def placement_group_schedule():\n",
    "    return 2\n",
    "\n",
    "\n",
    "out = ray.get(placement_group_schedule.remote())\n",
    "print(out)\n",
    "\n",
    "# Remove placement group.\n",
    "remove_placement_group(pg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetching task results\n",
    "\n",
    "<img src=\"fetch_result_.png\">\n",
    "\n",
    "Note: If the owner is fetching the result from a different node than the one where the task was executed, the result is first copied to the local object store of the owner node and then returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object management and dependency resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drill down on how a task's dependencies are resolved - using the following example of simple batch inference:\n",
    "\n",
    "- we load a model\n",
    "- we use the model to make predictions on an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_model(size_mb):\n",
    "    weights = np.ones((1024, 1024, size_mb), dtype=np.uint8)\n",
    "    assert weights.nbytes / 1024**2 == size_mb\n",
    "    return weights\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def predict(model, input):\n",
    "    return model * input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with this simple implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load 1 GB model in memory\n",
    "model = load_model(1_000) \n",
    "\n",
    "# submit 3 tasks to the cluster\n",
    "futures = ray.get([predict.remote(model, i) for i in range(3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 `predict` tasks that will be submitted.\n",
    "\n",
    "- The owner of each task will need to go over all the task arguments and:\n",
    "    - check that all the arguments are available\n",
    "    - store a reference to all the available arguments in the plasma/shared object store or inprocess object store\n",
    "- In the case of our 1 GB \"model\", the owner will make use of the shared object store given it exceeds the 100KB limit of the inprocess object store\n",
    "- Each owner will create a copy of the model and produce an object reference to use as the argument for the task\n",
    "- Each owner process will now execute their task\n",
    "\n",
    "The outcome is that we have made 3 copies of the model in the shared object store.\n",
    "\n",
    "Instead to save on memory, we should use the `ray.put` API to store the model in the shared object store and pass the reference to the model as an argument to the task.\n",
    "\n",
    "Here is the optimized implementation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the model in the object store and get a reference to it\n",
    "model_ref = ray.put(model)\n",
    "\n",
    "# submit 3 tasks to the cluster using the same model reference\n",
    "futures = ray.get([predict.remote(model_ref, i) for i in range(3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 feet view of ray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting debug logs\n",
    "\n",
    "Given the below code, we can inspect the debug logs to see what is happening under the hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_model(size_mb):\n",
    "    weights = np.ones((1024, 1024, size_mb), dtype=np.uint8)\n",
    "    assert weights.nbytes / 1024**2 == size_mb\n",
    "    return weights\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def predict(model, input):\n",
    "    return model * input\n",
    "\n",
    "\n",
    "model = load_model(size_mb=1000)\n",
    "obj_ref = predict.remote(model, 1)\n",
    "result = ray.get(obj_ref)  # c8ef45ccd0112571ffffffffffffffffffffffff0100000001000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the the worker process debug logs parsed into pandas, color-categorized and annotated\n",
    "\n",
    "<img src=\"debug_logs_with_legend.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fault Tolerance of Ray Tasks and Objects\n",
    "\n",
    "- If a task raises an application-level exception, the task will fail and the exception will be propagated to the caller.\n",
    "- If instead a system-level failures, i.e the worker process executing the task crashes then:\n",
    "    - Ray will rerun the task until either the task succeeds or the maximum number of retries is exceeded. \n",
    "        - The default number of retries is 3 and can be overridden by specifying max_retries in the @ray.remote decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# application-level failure flakiness but with infinite retries\n",
    "import ray\n",
    "import pickle\n",
    "\n",
    "def write_x(val):\n",
    "    with open(\"x.pkl\", \"wb\") as f:\n",
    "        pickle.dump({\"x\": val}, f)    \n",
    "\n",
    "def read_x():\n",
    "    with open(\"x.pkl\", \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    return data[\"x\"]\n",
    "\n",
    "# start with x = 0 to force failure\n",
    "write_x(0)\n",
    "\n",
    "@ray.remote(max_retries=-1) # infinite retries\n",
    "def flaky_app_task():\n",
    "    \"\"\"Reads x, increments it by 1, writes it and fails, next retry it should pass.\"\"\"\n",
    "    x = read_x()\n",
    "    if x % 2 == 0:\n",
    "        x += 1\n",
    "        write_x(1)\n",
    "        raise ValueError(\"x is even - that's odd!\")\n",
    "    return 1\n",
    "\n",
    "try:\n",
    "    out = ray.get(flaky_app_task.remote())\n",
    "except ray.exceptions.RayTaskError:\n",
    "    print(\"application-level exceptions shortcircuit retries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** You can enable retries on application-level exceptions you need to set `retry_exceptions=True` or specify a list of exceptions\n",
    "\n",
    "Make sure your task is **idempotent** to avoid side-effects due to retries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system-level failure flakiness but with infinite retries\n",
    "import sys\n",
    "import ray\n",
    "import pickle\n",
    "\n",
    "def write_x(val):\n",
    "    with open(\"x.pkl\", \"wb\") as f:\n",
    "        pickle.dump({\"x\": val}, f)    \n",
    "\n",
    "def read_x():\n",
    "    with open(\"x.pkl\", \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    return data[\"x\"]\n",
    "\n",
    "# start with x = 0 to force failure\n",
    "write_x(0)\n",
    "\n",
    "@ray.remote(max_retries=-1) # infinite retries\n",
    "def flaky_sys_task():\n",
    "    \"\"\"Reads x, increments it by 1, writes it and fails, next retry it should pass.\"\"\"\n",
    "    x = read_x()\n",
    "    if x % 2 == 0:\n",
    "        x += 1\n",
    "        write_x(1)\n",
    "        raise sys.exit(1)\n",
    "    return 1\n",
    "\n",
    "# never raises an error given retries eventually succeed\n",
    "out = ray.get(flaky_sys_task.remote())\n",
    "print(\"returned\", out, \"after retrying worker failure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below diagram shows the fault tolerance of ray objects - taken from https://www.usenix.org/system/files/nsdi21-wang.pdf\n",
    "\n",
    "<img src=\"object_fault_tolerance.png\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When an object value is lost from the object store, such as during node failures:\n",
    "    - Ray will use lineage reconstruction to recover the object.\n",
    "        - Ray will first automatically attempt to recover the value by looking for copies of the same object on other nodes.\n",
    "        - If none are found, then Ray will automatically recover the value by re-executing the task that previously created the value. \n",
    "        - Arguments to the task are recursively reconstructed through the same mechanism.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
